{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Model Evaluation Template\n",
    "\n",
    "**Purpose:** Comprehensive evaluation of dialogue generation models with automated metrics.\n",
    "\n",
    "**What this notebook evaluates:**\n",
    "- Dialogue Quality (perplexity, diversity, repetition)\n",
    "- Emotion Classification (accuracy, F1-score)\n",
    "- Emotional Consistency (transitions, appropriateness)\n",
    "- Affection Progression (dating-specific metrics)\n",
    "\n",
    "**How to use:**\n",
    "1. Copy this notebook and rename it (e.g., `eval_model_v1.ipynb`)\n",
    "2. Update the checkpoint paths in Section 2\n",
    "3. Run all cells sequentially\n",
    "4. Results are saved automatically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in correct directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Should be in notebooks/ directory\n",
    "if Path.cwd().name != 'notebooks':\n",
    "    print(f\"âš ï¸  Current directory: {Path.cwd()}\")\n",
    "    print(\"âš ï¸  This notebook should be run from the notebooks/ directory\")\n",
    "else:\n",
    "    print(\"âœ“ Running from correct directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "print(\"âœ“ Path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# Evaluation modules\n",
    "from src.evaluation import (\n",
    "    DialogueEvaluator,\n",
    "    DialogueQualityMetrics,\n",
    "    EmotionClassificationMetrics,\n",
    "    EmotionTransitionAnalyzer,\n",
    "    EmotionResponseAppropriatenessEvaluator,\n",
    "    AffectionTracker,\n",
    "    evaluate_emotional_consistency\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Models\n",
    "\n",
    "**âš ï¸ CUSTOMIZE THIS SECTION:**\n",
    "- Update `DIALOGUE_CHECKPOINT` with your model path\n",
    "- Update `EMOTION_CHECKPOINT` if you have an emotion detector\n",
    "- Set `USE_EMOTION_DETECTOR = False` if you don't have one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CUSTOMIZE THESE PATHS ====================\n",
    "\n",
    "# Path to your dialogue model checkpoint\n",
    "DIALOGUE_CHECKPOINT = \"../checkpoints/dialogue_model\"  # UPDATE THIS\n",
    "\n",
    "# Path to your emotion detector checkpoint (optional)\n",
    "EMOTION_CHECKPOINT = \"../checkpoints/emotion_model\"  # UPDATE THIS\n",
    "\n",
    "# Set to False if you don't have an emotion detector\n",
    "USE_EMOTION_DETECTOR = True  # Set to False to skip emotion evaluation\n",
    "\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dialogue model\n",
    "print(f\"Loading dialogue model from: {DIALOGUE_CHECKPOINT}\")\n",
    "\n",
    "try:\n",
    "    dialogue_tokenizer = AutoTokenizer.from_pretrained(DIALOGUE_CHECKPOINT)\n",
    "    dialogue_model = AutoModelForCausalLM.from_pretrained(DIALOGUE_CHECKPOINT)\n",
    "    dialogue_model.to(device)\n",
    "    dialogue_model.eval()\n",
    "    print(\"âœ“ Dialogue model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dialogue model: {e}\")\n",
    "    print(\"Please update DIALOGUE_CHECKPOINT path above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotion detector (optional)\n",
    "emotion_detector = None\n",
    "\n",
    "if USE_EMOTION_DETECTOR:\n",
    "    print(f\"Loading emotion detector from: {EMOTION_CHECKPOINT}\")\n",
    "    try:\n",
    "        emotion_detector = AutoModelForSequenceClassification.from_pretrained(EMOTION_CHECKPOINT)\n",
    "        emotion_detector.to(device)\n",
    "        emotion_detector.eval()\n",
    "        print(\"âœ“ Emotion detector loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading emotion detector: {e}\")\n",
    "        print(\"Setting USE_EMOTION_DETECTOR = False\")\n",
    "        USE_EMOTION_DETECTOR = False\n",
    "else:\n",
    "    print(\"âŠ˜ Emotion detector disabled - will skip emotion metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = DialogueEvaluator(\n",
    "    dialogue_model=dialogue_model,\n",
    "    tokenizer=dialogue_tokenizer,\n",
    "    emotion_detector=emotion_detector,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"âœ“ Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Quick Test - Generate Sample Responses\n",
    "\n",
    "Test your model with a few sample prompts to verify it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test prompts\n",
    "test_prompts = [\n",
    "    \"Hi! How are you doing today?\",\n",
    "    \"What do you like to do for fun?\",\n",
    "    \"Tell me something interesting about yourself.\",\n",
    "]\n",
    "\n",
    "print(\"Generating sample responses...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = evaluator.generate_response(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9\n",
    "    )[0]\n",
    "    \n",
    "    print(f\"User: {prompt}\")\n",
    "    print(f\"Bot:  {response}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prepare Test Data\n",
    "\n",
    "Choose one of the following options:\n",
    "- **Option A:** Use sample contexts (included below)\n",
    "- **Option B:** Load from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Use Sample Contexts (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample contexts for testing\n",
    "test_contexts = [\n",
    "    \"Hi! How are you doing today?\",\n",
    "    \"What do you like to do for fun?\",\n",
    "    \"Tell me about your favorite hobbies.\",\n",
    "    \"What kind of music do you enjoy?\",\n",
    "    \"Do you like to travel? Where would you go?\",\n",
    "    \"What's your idea of a perfect day?\",\n",
    "    \"What makes you laugh?\",\n",
    "    \"What are you passionate about?\",\n",
    "    \"What's something you've always wanted to try?\",\n",
    "    \"How do you usually spend your weekends?\",\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Using {len(test_contexts)} sample contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Load from File (Uncomment to use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and modify this cell to load from file\n",
    "# \n",
    "# # For JSON file\n",
    "# with open('../data/test_contexts.json', 'r') as f:\n",
    "#     test_contexts = json.load(f)\n",
    "# \n",
    "# # For text file (one context per line)\n",
    "# # with open('../data/test_contexts.txt', 'r') as f:\n",
    "# #     test_contexts = [line.strip() for line in f if line.strip()]\n",
    "# \n",
    "# print(f\"âœ“ Loaded {len(test_contexts)} contexts from file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run Evaluation\n",
    "\n",
    "This will:\n",
    "1. Generate responses for all test contexts\n",
    "2. Compute all metrics\n",
    "3. Store results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation parameters\n",
    "generation_kwargs = {\n",
    "    'max_length': 100,\n",
    "    'temperature': 1.0,\n",
    "    'top_p': 0.9\n",
    "}\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "print(f\"Generation parameters: {generation_kwargs}\")\n",
    "print()\n",
    "\n",
    "results = evaluator.evaluate_from_generations(\n",
    "    contexts=test_contexts,\n",
    "    generate_kwargs=generation_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted summary\n",
    "evaluator.print_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Detailed Analysis - Dialogue Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Diversity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dialogue quality metrics\n",
    "dq = results['dialogue_quality']\n",
    "\n",
    "# Create DataFrame for display\n",
    "diversity_data = {\n",
    "    'Metric': ['Distinct-1', 'Distinct-2', 'Distinct-3'],\n",
    "    'Score': [\n",
    "        dq.get('distinct_1', 0),\n",
    "        dq.get('distinct_2', 0),\n",
    "        dq.get('distinct_3', 0)\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Unique words / total words',\n",
    "        'Unique bigrams / total bigrams',\n",
    "        'Unique trigrams / total trigrams'\n",
    "    ]\n",
    "}\n",
    "\n",
    "diversity_df = pd.DataFrame(diversity_data)\n",
    "print(\"Lexical Diversity Metrics:\")\n",
    "display(diversity_df)\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\nðŸ“Š Interpretation Guide:\")\n",
    "print(\"  â€¢ Distinct-1 > 0.3: Good vocabulary diversity\")\n",
    "print(\"  â€¢ Distinct-2 > 0.6: Good phrase diversity\")\n",
    "print(\"  â€¢ Higher is better (max = 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize diversity\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "metrics = ['distinct_1', 'distinct_2', 'distinct_3']\n",
    "values = [dq.get(m, 0) for m in metrics]\n",
    "labels = ['Distinct-1\\n(words)', 'Distinct-2\\n(bigrams)', 'Distinct-3\\n(trigrams)']\n",
    "\n",
    "bars = ax.bar(labels, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Lexical Diversity Metrics', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=0.3, color='gray', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Perplexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'perplexity' in dq:\n",
    "    ppl = dq['perplexity']\n",
    "    \n",
    "    print(\"Perplexity Metrics:\")\n",
    "    print(f\"  Mean: {ppl['mean_perplexity']:.2f}\")\n",
    "    print(f\"  Std:  {ppl['std_perplexity']:.2f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Interpretation:\")\n",
    "    if ppl['mean_perplexity'] < 20:\n",
    "        print(\"  âœ“ Excellent - Model is very confident\")\n",
    "    elif ppl['mean_perplexity'] < 50:\n",
    "        print(\"  âœ“ Good - Model is reasonably confident\")\n",
    "    elif ppl['mean_perplexity'] < 100:\n",
    "        print(\"  âš  Fair - Model has moderate uncertainty\")\n",
    "    else:\n",
    "        print(\"  âš  Poor - Model may need more training\")\n",
    "    \n",
    "    # Plot distribution if per-sample available\n",
    "    if 'perplexities' in ppl:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.hist(ppl['perplexities'], bins=20, color='#45B7D1', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(ppl['mean_perplexity'], color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {ppl[\"mean_perplexity\"]:.2f}')\n",
    "        ax.set_xlabel('Perplexity', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title('Perplexity Distribution', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"âŠ˜ Perplexity not computed (requires model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Repetition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetition = dq.get('mean_repetition', 0)\n",
    "\n",
    "print(f\"Mean Repetition Ratio: {repetition:.4f}\")\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "if repetition < 0.05:\n",
    "    print(\"  âœ“ Excellent - Very little repetition\")\n",
    "elif repetition < 0.1:\n",
    "    print(\"  âœ“ Good - Acceptable repetition\")\n",
    "elif repetition < 0.2:\n",
    "    print(\"  âš  Fair - Some repetition present\")\n",
    "else:\n",
    "    print(\"  âš  Poor - High repetition, model may be stuck\")\n",
    "\n",
    "# Gauge chart\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.barh(['Repetition'], [repetition], color='#FF6B6B' if repetition > 0.1 else '#4ECDC4')\n",
    "ax.set_xlim(0, 0.3)\n",
    "ax.set_xlabel('Repetition Ratio', fontsize=12)\n",
    "ax.set_title('Self-Repetition Analysis', fontsize=14, fontweight='bold')\n",
    "ax.axvline(0.1, color='orange', linestyle='--', alpha=0.7, label='Target threshold')\n",
    "ax.text(repetition, 0, f'  {repetition:.4f}', va='center', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Response Length Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_stats = {\n",
    "    'Mean': dq.get('mean_length', 0),\n",
    "    'Std': dq.get('std_length', 0),\n",
    "    'Min': dq.get('min_length', 0),\n",
    "    'Max': dq.get('max_length', 0),\n",
    "    'Median': dq.get('median_length', 0)\n",
    "}\n",
    "\n",
    "length_df = pd.DataFrame([length_stats])\n",
    "print(\"Response Length Statistics (words):\")\n",
    "display(length_df)\n",
    "\n",
    "# Box plot would go here if we had per-sample data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "stats = ['Mean', 'Median', 'Min', 'Max']\n",
    "vals = [length_stats[s] for s in stats]\n",
    "colors = ['#4ECDC4', '#45B7D1', '#95E1D3', '#F38181']\n",
    "bars = ax.bar(stats, vals, color=colors)\n",
    "ax.set_ylabel('Words', fontsize=12)\n",
    "ax.set_title('Response Length Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars, vals):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Emotion Analysis (if available)\n",
    "\n",
    "Skip this section if `USE_EMOTION_DETECTOR = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_EMOTION_DETECTOR and 'emotion_distribution' in results:\n",
    "    emotion_dist = results['emotion_distribution']\n",
    "    \n",
    "    print(\"Emotion Distribution:\")\n",
    "    for emotion, count in sorted(emotion_dist.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / sum(emotion_dist.values())) * 100\n",
    "        print(f\"  {emotion:12s}: {count:3d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Pie chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = sns.color_palette('husl', len(emotion_dist))\n",
    "    ax.pie(emotion_dist.values(), labels=emotion_dist.keys(), autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90)\n",
    "    ax.set_title('Generated Response Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŠ˜ Emotion analysis not available (emotion detector not loaded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"../results/evaluation_{timestamp}.json\"\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "evaluator.save_results(results, output_path)\n",
    "\n",
    "print(f\"âœ“ Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for easy comparison\n",
    "summary = {\n",
    "    'Timestamp': timestamp,\n",
    "    'Model': DIALOGUE_CHECKPOINT,\n",
    "    'Num Samples': results.get('num_samples', 0),\n",
    "    'Distinct-1': dq.get('distinct_1', 0),\n",
    "    'Distinct-2': dq.get('distinct_2', 0),\n",
    "    'Distinct-3': dq.get('distinct_3', 0),\n",
    "    'Mean Repetition': dq.get('mean_repetition', 0),\n",
    "    'Mean Length': dq.get('mean_length', 0),\n",
    "}\n",
    "\n",
    "if 'perplexity' in dq:\n",
    "    summary['Mean Perplexity'] = dq['perplexity']['mean_perplexity']\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Save summary to CSV for tracking across evaluations\n",
    "summary_csv_path = \"../results/evaluation_history.csv\"\n",
    "if Path(summary_csv_path).exists():\n",
    "    # Append to existing\n",
    "    history = pd.read_csv(summary_csv_path)\n",
    "    history = pd.concat([history, summary_df], ignore_index=True)\n",
    "    history.to_csv(summary_csv_path, index=False)\n",
    "else:\n",
    "    # Create new\n",
    "    summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Summary appended to: {summary_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Compare with Previous Evaluations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation history\n",
    "history_path = \"../results/evaluation_history.csv\"\n",
    "\n",
    "if Path(history_path).exists():\n",
    "    history = pd.read_csv(history_path)\n",
    "    \n",
    "    print(f\"Found {len(history)} previous evaluations\")\n",
    "    print(\"\\nRecent Evaluations:\")\n",
    "    display(history.tail(5))\n",
    "    \n",
    "    # Plot trends if multiple evaluations exist\n",
    "    if len(history) > 1:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Diversity trends\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(history.index, history['Distinct-1'], 'o-', label='Distinct-1')\n",
    "        ax.plot(history.index, history['Distinct-2'], 's-', label='Distinct-2')\n",
    "        ax.plot(history.index, history['Distinct-3'], '^-', label='Distinct-3')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('Diversity Trends')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Repetition trends\n",
    "        ax = axes[0, 1]\n",
    "        ax.plot(history.index, history['Mean Repetition'], 'o-', color='#FF6B6B')\n",
    "        ax.set_ylabel('Repetition Ratio')\n",
    "        ax.set_title('Repetition Trends')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Length trends\n",
    "        ax = axes[1, 0]\n",
    "        ax.plot(history.index, history['Mean Length'], 'o-', color='#4ECDC4')\n",
    "        ax.set_ylabel('Words')\n",
    "        ax.set_title('Response Length Trends')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Perplexity trends (if available)\n",
    "        ax = axes[1, 1]\n",
    "        if 'Mean Perplexity' in history.columns:\n",
    "            ax.plot(history.index, history['Mean Perplexity'], 'o-', color='#45B7D1')\n",
    "            ax.set_ylabel('Perplexity')\n",
    "            ax.set_title('Perplexity Trends')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No perplexity data', ha='center', va='center')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"âŠ˜ No evaluation history found\")\n",
    "    print(\"Run this notebook multiple times to see trends!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Evaluation Complete! ðŸŽ‰\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review the metrics** - Focus on areas that need improvement\n",
    "2. **Compare with baselines** - Check the evaluation history\n",
    "3. **Iterate on training** - Use insights to improve your model\n",
    "4. **Re-evaluate** - Copy this notebook and run again after changes\n",
    "\n",
    "### Key Files Generated:\n",
    "- Detailed results: `results/evaluation_TIMESTAMP.json`\n",
    "- Summary history: `results/evaluation_history.csv`\n",
    "\n",
    "### Tips:\n",
    "- Good distinct-1 > 0.3, distinct-2 > 0.6\n",
    "- Keep repetition < 0.1\n",
    "- Lower perplexity = better (< 50 is good)\n",
    "- Track trends across multiple training runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
