{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MELD Dating Simulator Evaluation (Fixed) üé≠üìä\n",
    "\n",
    "**Purpose:** Comprehensive evaluation with FIXED generation parameters\n",
    "\n",
    "**What this notebook evaluates:**\n",
    "- üìà Multiple checkpoints from training automatically\n",
    "- üéØ Dialogue Quality (perplexity, diversity, repetition)\n",
    "- üìù Reference-Based Metrics (BLEU, ROUGE vs ground truth)\n",
    "- üé≠ Per-Character Performance (all 6 Friends characters)\n",
    "- üí≠ Per-Emotion Performance (7 emotion categories)\n",
    "- üèÜ Best Checkpoint Identification\n",
    "\n",
    "**Key Improvements:**\n",
    "- ‚úÖ Correct EOS token (`tokenizer.eos_token_id`)\n",
    "- ‚úÖ Repetition penalty (1.2) + no_repeat_ngram_size (3)\n",
    "- ‚úÖ Lower temperature (0.7 instead of 0.9)\n",
    "- ‚úÖ Reduced max_new_tokens (50 instead of 128)\n",
    "- ‚úÖ Token-based extraction (not regex)\n",
    "- ‚úÖ Speaker token stripping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in correct directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Should be in notebooks/MELD/ directory\n",
    "if Path.cwd().name == 'MELD':\n",
    "    print(\"‚úì Running from correct directory\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Current directory: {Path.cwd()}\")\n",
    "    print(\"‚ö†Ô∏è  This notebook should be run from the notebooks/MELD/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "\n",
    "print(\"‚úì Path configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "# NLP metrics\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "**‚ö†Ô∏è CUSTOMIZE THESE PATHS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CUSTOMIZE THESE PATHS ====================\n",
    "\n",
    "# Path to checkpoint directory (will auto-detect all checkpoints)\n",
    "CHECKPOINT_DIR = \"../../checkpoints/dating_sim_meld_fixed\"  # Update to your training output\n",
    "\n",
    "# Path to MELD instruction-formatted data\n",
    "DATA_PATH = \"../../data/processed/MELD/meld_dating_sim_instruct.csv\"\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = \"../../results/MELD\"\n",
    "\n",
    "# Sampling configuration\n",
    "SAMPLES_PER_GROUP = 5  # Samples per (character, emotion) combination\n",
    "\n",
    "# FIXED Generation parameters (matching training notebook 03c)\n",
    "GENERATION_CONFIG = {\n",
    "    'max_new_tokens': 50,    # Reduced from 128\n",
    "    'temperature': 0.7,      # Reduced from 0.9\n",
    "    'top_p': 0.9,\n",
    "    'do_sample': True\n",
    "}\n",
    "\n",
    "# ===============================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"  Data path: {DATA_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Samples per (character, emotion): {SAMPLES_PER_GROUP}\")\n",
    "print(f\"  Generation config: {GENERATION_CONFIG}\")\n",
    "print(f\"\\n‚úÖ Using FIXED generation parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Friends Character Personas\n",
    "\n",
    "Define personas used in generation (same as training notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character personas from training notebook\n",
    "CHARACTER_PERSONAS = {\n",
    "    'Chandler': \"You are Chandler Bing from Friends. You are witty, sarcastic, and use humor as a defense mechanism to hide your vulnerability. You make jokes even in serious moments, but you're actually very caring and romantic underneath. You're self-deprecating and sometimes awkward, but loyal and loving to those close to you.\",\n",
    "    \n",
    "    'Monica': \"You are Monica Geller from Friends. You are organized, competitive, and love to be in control. You're nurturing and care deeply about the people in your life. You want commitment and stability in relationships. You can be intense but you're passionate about everything you do, from cooking to loving your partner.\",\n",
    "    \n",
    "    'Ross': \"You are Ross Geller from Friends. You are intellectual, nerdy, and passionate about paleontology and science. You tend to overthink things and can be awkward in romantic situations. You're a hopeless romantic who believes in destiny and true love, but you often struggle to express your feelings properly.\",\n",
    "    \n",
    "    'Rachel': \"You are Rachel Green from Friends. You are fashion-focused, fun, and flirty. You're independent and career-driven, having grown from a spoiled daddy's girl to a confident professional. You value friendship and are loyal to those you care about. You're charming and know how to make people feel special.\",\n",
    "    \n",
    "    'Joey': \"You are Joey Tribbiani from Friends. You are confident, charming, and simple in the best way. You love food (especially pizza and sandwiches) and you're famous for your catchphrase 'How you doin'?' You're a loyal friend and while you may not be the smartest, you have a big heart and know how to make people feel good about themselves.\",\n",
    "    \n",
    "    'Phoebe': \"You are Phoebe Buffay from Friends. You are quirky, spiritual, and unconventionally wise. You're honest to a fault and say what's on your mind. You have a mysterious past but maintain an optimistic outlook. You're free-spirited and bring unique perspectives to every situation. You believe in karma, auras, and following your heart.\"\n",
    "}\n",
    "\n",
    "# Dating scenarios\n",
    "DATING_SCENARIOS = [\n",
    "    \"You're on a casual coffee date at Central Perk, the cozy coffee shop.\",\n",
    "    \"You're having a romantic dinner date at a nice restaurant.\",\n",
    "    \"You're taking a walk together and having a deep conversation.\",\n",
    "    \"You're hanging out at your apartment, enjoying each other's company.\",\n",
    "    \"You're on a fun date doing something adventurous together.\",\n",
    "    \"You're having a heart-to-heart conversation about your relationship.\",\n",
    "    \"You're flirting and getting to know each other better.\",\n",
    "    \"You're spending a quiet evening together, just talking and connecting.\",\n",
    "]\n",
    "\n",
    "print(\"‚úì Character personas and scenarios loaded\")\n",
    "print(f\"  Characters: {', '.join(CHARACTER_PERSONAS.keys())}\")\n",
    "print(f\"  Scenarios: {len(DATING_SCENARIOS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full MELD dataset\n",
    "print(f\"Loading data from: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df)} total examples\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data distribution\n",
    "print(\"=\"*80)\n",
    "print(\"Character Distribution\")\n",
    "print(\"=\"*80)\n",
    "char_counts = df['character'].value_counts()\n",
    "for char, count in char_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{char:15s}: {count:5d} ({percentage:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Emotion Distribution\")\n",
    "print(\"=\"*80)\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "for emotion, count in emotion_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{emotion:15s}: {count:5d} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling\n",
    "\n",
    "Create balanced test set with N samples per (character, emotion) combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_sample(df, samples_per_group=5, main_characters=None):\n",
    "    \"\"\"\n",
    "    Create stratified sample with balanced (character, emotion) coverage.\n",
    "    \"\"\"\n",
    "    # Filter to main characters if specified\n",
    "    if main_characters is not None:\n",
    "        df = df[df['character'].isin(main_characters)]\n",
    "    \n",
    "    # Group by character and emotion\n",
    "    grouped = df.groupby(['character', 'emotion'])\n",
    "    \n",
    "    # Sample from each group\n",
    "    samples = []\n",
    "    for (char, emotion), group in grouped:\n",
    "        n = min(samples_per_group, len(group))\n",
    "        if n > 0:\n",
    "            samples.append(group.sample(n, random_state=42))\n",
    "    \n",
    "    return pd.concat(samples, ignore_index=True)\n",
    "\n",
    "# Main 6 Friends characters\n",
    "MAIN_CHARACTERS = ['Chandler', 'Monica', 'Ross', 'Rachel', 'Joey', 'Phoebe']\n",
    "\n",
    "# Create stratified test sample\n",
    "test_df = create_stratified_sample(\n",
    "    df,\n",
    "    samples_per_group=SAMPLES_PER_GROUP,\n",
    "    main_characters=MAIN_CHARACTERS\n",
    ")\n",
    "\n",
    "print(f\"‚úì Created stratified test set: {len(test_df)} samples\")\n",
    "print(f\"\\nDistribution:\")\n",
    "print(test_df.groupby(['character', 'emotion']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Checkpoint Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoints(checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Auto-detect all checkpoint folders in directory.\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_dir)\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Checkpoint directory not found: {checkpoint_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all checkpoint-* folders\n",
    "    checkpoints = sorted(\n",
    "        checkpoint_path.glob(\"checkpoint-*\"),\n",
    "        key=lambda p: int(p.name.split('-')[1])\n",
    "    )\n",
    "    \n",
    "    # Add final model if exists\n",
    "    final_path = checkpoint_path / \"final\"\n",
    "    if final_path.exists():\n",
    "        checkpoints.append(final_path)\n",
    "    \n",
    "    return checkpoints\n",
    "\n",
    "# Discover checkpoints\n",
    "checkpoints = find_checkpoints(CHECKPOINT_DIR)\n",
    "\n",
    "if not checkpoints:\n",
    "    print(\"‚ùå No checkpoints found!\")\n",
    "    print(f\"Please check the path: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úì Found {len(checkpoints)} checkpoint(s):\")\n",
    "    for i, cp in enumerate(checkpoints, 1):\n",
    "        print(f\"  {i}. {cp.name}\")\n",
    "    \n",
    "    # Estimate evaluation time\n",
    "    samples_per_checkpoint = len(test_df)\n",
    "    estimated_minutes = len(checkpoints) * samples_per_checkpoint * 0.05\n",
    "    print(f\"\\n‚è±Ô∏è  Estimated evaluation time: ~{estimated_minutes:.1f} minutes\")\n",
    "    print(f\"   ({len(checkpoints)} checkpoints √ó {samples_per_checkpoint} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. FIXED MELD Generation Function üîß\n",
    "\n",
    "**Uses corrected parameters matching training notebook 03c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meld_response_fixed(model, tokenizer, character, emotion, context, scenario, **gen_kwargs):\n",
    "    \"\"\"\n",
    "    Generate response using FIXED parameters (matches training notebook 03c).\n",
    "    \n",
    "    Fixes applied:\n",
    "    - Correct EOS token (tokenizer.eos_token_id)\n",
    "    - Repetition penalty (1.2)\n",
    "    - No repeat ngram size (3)\n",
    "    - Token-based extraction\n",
    "    - Speaker token stripping\n",
    "    \"\"\"\n",
    "    # Get persona description\n",
    "    persona_desc = CHARACTER_PERSONAS.get(\n",
    "        character,\n",
    "        f\"You are {character} from Friends.\"\n",
    "    )\n",
    "    \n",
    "    # Build system prompt (matches training format)\n",
    "    system_content = f\"\"\"{persona_desc}\n",
    "\n",
    "Scenario: {scenario}\n",
    "The user seems to be feeling: {emotion}\"\"\"\n",
    "    \n",
    "    # User message with conversation context\n",
    "    user_content = f\"Conversation:\\n{context}\"\n",
    "    \n",
    "    # Build messages for LLaMA 3.1 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template WITH generation prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate with FIXED parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # FIX: Correct EOS token\n",
    "            repetition_penalty=1.2,  # FIX: Add repetition penalty\n",
    "            no_repeat_ngram_size=3,  # FIX: Prevent 3-gram repetition\n",
    "            **gen_kwargs\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated tokens (not the prompt)\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    \n",
    "    # Decode with skip_special_tokens\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Safety net: Remove any accidental speaker tokens\n",
    "    response = re.sub(r'<[^>]+>\\s*', '', response)\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    response = ' '.join(response.split())\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úì FIXED MELD generation function ready\")\n",
    "print(\"\\nKey improvements:\")\n",
    "print(\"  ‚Ä¢ Correct EOS token ID\")\n",
    "print(\"  ‚Ä¢ Repetition penalty: 1.2\")\n",
    "print(\"  ‚Ä¢ No repeat ngram size: 3\")\n",
    "print(\"  ‚Ä¢ Token-based extraction\")\n",
    "print(\"  ‚Ä¢ Speaker token stripping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Evaluation Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_scores(reference, hypothesis):\n",
    "    \"\"\"Compute BLEU-1, BLEU-2, BLEU-3, BLEU-4 scores.\"\"\"\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    return {\n",
    "        'bleu-1': sentence_bleu([ref_tokens], hyp_tokens, weights=(1,0,0,0), smoothing_function=smoothing),\n",
    "        'bleu-2': sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5,0.5,0,0), smoothing_function=smoothing),\n",
    "        'bleu-3': sentence_bleu([ref_tokens], hyp_tokens, weights=(0.33,0.33,0.33,0), smoothing_function=smoothing),\n",
    "        'bleu-4': sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothing),\n",
    "    }\n",
    "\n",
    "def compute_rouge_scores(reference, hypothesis):\n",
    "    \"\"\"Compute ROUGE-L score.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return {'rouge-l': scores['rougeL'].fmeasure}\n",
    "\n",
    "def compute_distinct_n(texts, n):\n",
    "    \"\"\"Compute distinct-n metric (lexical diversity).\"\"\"\n",
    "    all_ngrams = []\n",
    "    for text in texts:\n",
    "        tokens = text.lower().split()\n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    if len(all_ngrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(set(all_ngrams)) / len(all_ngrams)\n",
    "\n",
    "def compute_repetition_ratio(text):\n",
    "    \"\"\"Compute self-repetition ratio.\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    if len(tokens) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    bigrams = [tuple(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "    if len(bigrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return 1 - (len(set(bigrams)) / len(bigrams))\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts):\n",
    "    \"\"\"Compute perplexity for generated texts.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            total_loss += outputs.loss.item() * inputs['input_ids'].size(1)\n",
    "            total_tokens += inputs['input_ids'].size(1)\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "print(\"‚úì Metric functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Main Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoint(checkpoint_path, test_df, scenarios):\n",
    "    \"\"\"\n",
    "    Evaluate a single checkpoint on test data using FIXED generation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {checkpoint_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        checkpoint_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"‚úì Model loaded\")\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'checkpoint': checkpoint_path.name,\n",
    "        'samples': [],\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Generate responses for all test samples\n",
    "    print(f\"\\nGenerating {len(test_df)} responses with FIXED parameters...\")\n",
    "    generated_texts = []\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating\"):\n",
    "        # Get scenario\n",
    "        import random\n",
    "        random.seed(int(row.get('dialogue_id', idx)))\n",
    "        scenario = random.choice(scenarios)\n",
    "        \n",
    "        # Generate response with FIXED function\n",
    "        generated = generate_meld_response_fixed(\n",
    "            model, tokenizer,\n",
    "            row['character'],\n",
    "            row['emotion'],\n",
    "            row['context'],\n",
    "            scenario,\n",
    "            **GENERATION_CONFIG\n",
    "        )\n",
    "        \n",
    "        generated_texts.append(generated)\n",
    "        \n",
    "        # Compute reference-based metrics\n",
    "        bleu_scores = compute_bleu_scores(row['response'], generated)\n",
    "        rouge_scores = compute_rouge_scores(row['response'], generated)\n",
    "        \n",
    "        # Store sample result\n",
    "        results['samples'].append({\n",
    "            'character': row['character'],\n",
    "            'emotion': row['emotion'],\n",
    "            'context': row['context'],\n",
    "            'ground_truth': row['response'],\n",
    "            'generated': generated,\n",
    "            'bleu-1': bleu_scores['bleu-1'],\n",
    "            'bleu-2': bleu_scores['bleu-2'],\n",
    "            'bleu-3': bleu_scores['bleu-3'],\n",
    "            'bleu-4': bleu_scores['bleu-4'],\n",
    "            'rouge-l': rouge_scores['rouge-l'],\n",
    "            'length': len(generated.split()),\n",
    "            'repetition': compute_repetition_ratio(generated)\n",
    "        })\n",
    "    \n",
    "    # Compute aggregate metrics\n",
    "    print(\"\\nComputing aggregate metrics...\")\n",
    "    samples_df = pd.DataFrame(results['samples'])\n",
    "    \n",
    "    # Overall metrics\n",
    "    results['metrics']['overall'] = {\n",
    "        'bleu-1': samples_df['bleu-1'].mean(),\n",
    "        'bleu-2': samples_df['bleu-2'].mean(),\n",
    "        'bleu-3': samples_df['bleu-3'].mean(),\n",
    "        'bleu-4': samples_df['bleu-4'].mean(),\n",
    "        'rouge-l': samples_df['rouge-l'].mean(),\n",
    "        'distinct-1': compute_distinct_n(generated_texts, 1),\n",
    "        'distinct-2': compute_distinct_n(generated_texts, 2),\n",
    "        'distinct-3': compute_distinct_n(generated_texts, 3),\n",
    "        'mean_length': samples_df['length'].mean(),\n",
    "        'std_length': samples_df['length'].std(),\n",
    "        'mean_repetition': samples_df['repetition'].mean(),\n",
    "    }\n",
    "    \n",
    "    # Compute perplexity\n",
    "    print(\"Computing perplexity...\")\n",
    "    try:\n",
    "        perplexity = compute_perplexity(model, tokenizer, generated_texts[:50])\n",
    "        results['metrics']['overall']['perplexity'] = perplexity\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Perplexity computation failed: {e}\")\n",
    "        results['metrics']['overall']['perplexity'] = None\n",
    "    \n",
    "    # Per-character metrics\n",
    "    results['metrics']['per_character'] = {}\n",
    "    for char in samples_df['character'].unique():\n",
    "        char_df = samples_df[samples_df['character'] == char]\n",
    "        results['metrics']['per_character'][char] = {\n",
    "            'bleu-4': char_df['bleu-4'].mean(),\n",
    "            'rouge-l': char_df['rouge-l'].mean(),\n",
    "            'mean_length': char_df['length'].mean(),\n",
    "            'count': len(char_df)\n",
    "        }\n",
    "    \n",
    "    # Per-emotion metrics\n",
    "    results['metrics']['per_emotion'] = {}\n",
    "    for emotion in samples_df['emotion'].unique():\n",
    "        emotion_df = samples_df[samples_df['emotion'] == emotion]\n",
    "        results['metrics']['per_emotion'][emotion] = {\n",
    "            'bleu-4': emotion_df['bleu-4'].mean(),\n",
    "            'rouge-l': emotion_df['rouge-l'].mean(),\n",
    "            'mean_length': emotion_df['length'].mean(),\n",
    "            'count': len(emotion_df)\n",
    "        }\n",
    "    \n",
    "    print(\"‚úì Evaluation complete\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all checkpoints\n",
    "if checkpoints:\n",
    "    all_results = []\n",
    "    \n",
    "    for checkpoint in checkpoints:\n",
    "        results = evaluate_checkpoint(checkpoint, test_df, DATING_SCENARIOS)\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        interim_path = Path(OUTPUT_DIR) / f\"interim_{checkpoint.name}_{timestamp}.json\"\n",
    "        interim_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(interim_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n‚úì Interim results saved: {interim_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"All checkpoints evaluated!\")\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoints to evaluate\")\n",
    "    all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for result in all_results:\n",
    "        metrics = result['metrics']['overall']\n",
    "        summary_data.append({\n",
    "            'Checkpoint': result['checkpoint'],\n",
    "            'BLEU-4': f\"{metrics['bleu-4']:.4f}\",\n",
    "            'ROUGE-L': f\"{metrics['rouge-l']:.4f}\",\n",
    "            'Distinct-1': f\"{metrics['distinct-1']:.4f}\",\n",
    "            'Distinct-2': f\"{metrics['distinct-2']:.4f}\",\n",
    "            'Perplexity': f\"{metrics['perplexity']:.2f}\" if metrics['perplexity'] else 'N/A',\n",
    "            'Repetition': f\"{metrics['mean_repetition']:.4f}\",\n",
    "            'Avg Length': f\"{metrics['mean_length']:.1f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"Overall Metrics Comparison:\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Checkpoint Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    print(\"Best Checkpoint per Metric:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    metrics_to_check = [\n",
    "        ('bleu-4', 'higher'),\n",
    "        ('rouge-l', 'higher'),\n",
    "        ('distinct-1', 'higher'),\n",
    "        ('distinct-2', 'higher'),\n",
    "        ('perplexity', 'lower'),\n",
    "        ('mean_repetition', 'lower')\n",
    "    ]\n",
    "    \n",
    "    for metric, direction in metrics_to_check:\n",
    "        if direction == 'higher':\n",
    "            best = max(all_results, key=lambda r: r['metrics']['overall'][metric])\n",
    "        else:\n",
    "            valid_results = [r for r in all_results if r['metrics']['overall'].get(metric) is not None]\n",
    "            if valid_results:\n",
    "                best = min(valid_results, key=lambda r: r['metrics']['overall'][metric])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        value = best['metrics']['overall'][metric]\n",
    "        print(f\"{metric:20s}: {best['checkpoint']:20s} ({value:.4f})\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Visualizations\n",
    "\n",
    "### Checkpoint Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results and len(all_results) > 1:\n",
    "    checkpoint_names = [r['checkpoint'] for r in all_results]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # BLEU scores\n",
    "    ax = axes[0, 0]\n",
    "    for n in [1, 2, 3, 4]:\n",
    "        values = [r['metrics']['overall'][f'bleu-{n}'] for r in all_results]\n",
    "        ax.plot(checkpoint_names, values, marker='o', label=f'BLEU-{n}')\n",
    "    ax.set_title('BLEU Scores Across Checkpoints', fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # ROUGE-L\n",
    "    ax = axes[0, 1]\n",
    "    rouge_scores = [r['metrics']['overall']['rouge-l'] for r in all_results]\n",
    "    ax.plot(checkpoint_names, rouge_scores, marker='o', color='purple', linewidth=2)\n",
    "    ax.set_title('ROUGE-L Across Checkpoints', fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Diversity\n",
    "    ax = axes[0, 2]\n",
    "    distinct1 = [r['metrics']['overall']['distinct-1'] for r in all_results]\n",
    "    distinct2 = [r['metrics']['overall']['distinct-2'] for r in all_results]\n",
    "    ax.plot(checkpoint_names, distinct1, marker='o', label='Distinct-1')\n",
    "    ax.plot(checkpoint_names, distinct2, marker='s', label='Distinct-2')\n",
    "    ax.set_title('Diversity Across Checkpoints', fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Perplexity\n",
    "    ax = axes[1, 0]\n",
    "    perplexities = [r['metrics']['overall']['perplexity'] for r in all_results if r['metrics']['overall']['perplexity']]\n",
    "    if perplexities:\n",
    "        ax.plot(checkpoint_names[:len(perplexities)], perplexities, marker='o', color='red', linewidth=2)\n",
    "        ax.set_title('Perplexity Across Checkpoints', fontweight='bold')\n",
    "        ax.set_ylabel('Perplexity (lower is better)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No perplexity data', ha='center', va='center')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Repetition\n",
    "    ax = axes[1, 1]\n",
    "    repetitions = [r['metrics']['overall']['mean_repetition'] for r in all_results]\n",
    "    ax.plot(checkpoint_names, repetitions, marker='o', color='orange', linewidth=2)\n",
    "    ax.set_title('Repetition Ratio Across Checkpoints', fontweight='bold')\n",
    "    ax.set_ylabel('Repetition (lower is better)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Response length\n",
    "    ax = axes[1, 2]\n",
    "    lengths = [r['metrics']['overall']['mean_length'] for r in all_results]\n",
    "    ax.plot(checkpoint_names, lengths, marker='o', color='green', linewidth=2)\n",
    "    ax.set_title('Mean Response Length Across Checkpoints', fontweight='bold')\n",
    "    ax.set_ylabel('Words')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif all_results:\n",
    "    print(\"‚äò Only one checkpoint - skipping comparison plots\")\n",
    "else:\n",
    "    print(\"‚ùå No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Character Performance (Best Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    best_result = max(all_results, key=lambda r: r['metrics']['overall']['bleu-4'])\n",
    "    \n",
    "    print(f\"Per-Character Performance ({best_result['checkpoint']}):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    per_char = best_result['metrics']['per_character']\n",
    "    \n",
    "    characters = list(per_char.keys())\n",
    "    bleu4_scores = [per_char[c]['bleu-4'] for c in characters]\n",
    "    rougel_scores = [per_char[c]['rouge-l'] for c in characters]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # BLEU-4 by character\n",
    "    ax = axes[0]\n",
    "    ax.bar(characters, bleu4_scores, color='skyblue', edgecolor='black')\n",
    "    ax.set_title('BLEU-4 Score by Character', fontweight='bold')\n",
    "    ax.set_ylabel('BLEU-4')\n",
    "    ax.set_xlabel('Character')\n",
    "    for i, v in enumerate(bleu4_scores):\n",
    "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # ROUGE-L by character\n",
    "    ax = axes[1]\n",
    "    ax.bar(characters, rougel_scores, color='lightcoral', edgecolor='black')\n",
    "    ax.set_title('ROUGE-L Score by Character', fontweight='bold')\n",
    "    ax.set_ylabel('ROUGE-L')\n",
    "    ax.set_xlabel('Character')\n",
    "    for i, v in enumerate(rougel_scores):\n",
    "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Emotion Performance (Best Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    best_result = max(all_results, key=lambda r: r['metrics']['overall']['bleu-4'])\n",
    "    \n",
    "    print(f\"Per-Emotion Performance ({best_result['checkpoint']}):\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    per_emotion = best_result['metrics']['per_emotion']\n",
    "    \n",
    "    emotions = list(per_emotion.keys())\n",
    "    bleu4_scores = [per_emotion[e]['bleu-4'] for e in emotions]\n",
    "    rougel_scores = [per_emotion[e]['rouge-l'] for e in emotions]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # BLEU-4 by emotion\n",
    "    ax = axes[0]\n",
    "    ax.bar(emotions, bleu4_scores, color='lightgreen', edgecolor='black')\n",
    "    ax.set_title('BLEU-4 Score by Emotion', fontweight='bold')\n",
    "    ax.set_ylabel('BLEU-4')\n",
    "    ax.set_xlabel('Emotion')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    for i, v in enumerate(bleu4_scores):\n",
    "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # ROUGE-L by emotion\n",
    "    ax = axes[1]\n",
    "    ax.bar(emotions, rougel_scores, color='plum', edgecolor='black')\n",
    "    ax.set_title('ROUGE-L Score by Emotion', fontweight='bold')\n",
    "    ax.set_ylabel('ROUGE-L')\n",
    "    ax.set_xlabel('Emotion')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    for i, v in enumerate(rougel_scores):\n",
    "        ax.text(i, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Qualitative Examples\n",
    "\n",
    "Show sample generations from the best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    best_result = max(all_results, key=lambda r: r['metrics']['overall']['bleu-4'])\n",
    "    \n",
    "    print(f\"Sample Generations from {best_result['checkpoint']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    samples_df = pd.DataFrame(best_result['samples'])\n",
    "    \n",
    "    for char in MAIN_CHARACTERS:\n",
    "        char_samples = samples_df[samples_df['character'] == char]\n",
    "        if len(char_samples) > 0:\n",
    "            best_sample = char_samples.loc[char_samples['bleu-4'].idxmax()]\n",
    "            \n",
    "            print(f\"\\n{char} ({best_sample['emotion']}):\")\n",
    "            print(\"-\"*80)\n",
    "            context_display = best_sample['context'][:200] + \"...\" if len(best_sample['context']) > 200 else best_sample['context']\n",
    "            print(f\"Context: {context_display}\")\n",
    "            print(f\"\\nGround Truth: {best_sample['ground_truth']}\")\n",
    "            print(f\"Generated:    {best_sample['generated']}\")\n",
    "            print(f\"\\nBLEU-4: {best_sample['bleu-4']:.4f} | ROUGE-L: {best_sample['rouge-l']:.4f}\")\n",
    "            print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save full results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = output_path / f\"evaluation_fixed_{timestamp}.json\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Full results saved: {results_file}\")\n",
    "    \n",
    "    # Save summary CSV\n",
    "    if 'summary_df' in locals():\n",
    "        summary_file = output_path / f\"checkpoint_comparison_fixed_{timestamp}.csv\"\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        print(f\"‚úì Summary saved: {summary_file}\")\n",
    "    \n",
    "    # Save best checkpoint info\n",
    "    best_result = max(all_results, key=lambda r: r['metrics']['overall']['bleu-4'])\n",
    "    best_file = output_path / \"best_checkpoint_fixed.txt\"\n",
    "    \n",
    "    with open(best_file, 'w') as f:\n",
    "        f.write(f\"Best Checkpoint (by BLEU-4): {best_result['checkpoint']}\\n\")\n",
    "        f.write(f\"\\nMetrics:\\n\")\n",
    "        for metric, value in best_result['metrics']['overall'].items():\n",
    "            f.write(f\"  {metric}: {value}\\n\")\n",
    "    \n",
    "    print(f\"‚úì Best checkpoint info saved: {best_file}\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"All results saved!\")\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Evaluation Complete! üéâ\n",
    "\n",
    "### Summary:\n",
    "\n",
    "This notebook evaluated your MELD dating simulator with **FIXED generation parameters**:\n",
    "- ‚úÖ Correct EOS token\n",
    "- ‚úÖ Repetition controls\n",
    "- ‚úÖ Optimized temperature and token limits\n",
    "- ‚úÖ Clean token-based extraction\n",
    "- ‚úÖ Speaker token stripping\n",
    "\n",
    "### Expected Improvements:\n",
    "\n",
    "Compared to the original evaluation, responses should now:\n",
    "- ‚úÖ Be single-turn (no multi-conversation generation)\n",
    "- ‚úÖ Have minimal repetition (no \"Your haircut is attractive\" √ó 10)\n",
    "- ‚úÖ Contain no speaker tokens (no `<Joey>`, `<Rachel>`)\n",
    "- ‚úÖ Stop cleanly at sentence boundaries\n",
    "\n",
    "### Files Generated:\n",
    "- `results/MELD/evaluation_fixed_TIMESTAMP.json` - Full results\n",
    "- `results/MELD/checkpoint_comparison_fixed_TIMESTAMP.csv` - Summary table\n",
    "- `results/MELD/best_checkpoint_fixed.txt` - Best checkpoint info\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review qualitative examples** (Section 11) to verify response quality\n",
    "2. **Compare metrics** with previous evaluation (if available)\n",
    "3. **Use best checkpoint** for deployment or further testing\n",
    "4. **Continue training** if metrics are improving steadily"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
