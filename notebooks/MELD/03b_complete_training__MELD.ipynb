{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete MELD Training Pipeline - Friends Dating Simulator ðŸ¤–â¤ï¸\n",
    "\n",
    "**Purpose:** Self-contained notebook for training LLaMA 3.1 on MELD dating simulator\n",
    "\n",
    "**What this notebook does:**\n",
    "1. ðŸ“š Loads raw cleaned MELD data\n",
    "2. ðŸ“ Formats data with LLaMA 3.1 instruction template\n",
    "3. ðŸŽ¯ Fine-tunes LLaMA 3.1 with LoRA\n",
    "4. âœ… Tests generation with FIXED parameters (no repetition, proper stopping)\n",
    "\n",
    "**Key Features:**\n",
    "- Everything in one place (no external data dependencies)\n",
    "- Uses correct LLaMA 3.1 chat template\n",
    "- Fixed generation function (proper EOS token, repetition penalty)\n",
    "- Immediate testing after training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install tqdm\n",
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn\n",
    "!pip3 install transformers\n",
    "!pip3 install datasets\n",
    "!pip3 install accelerate\n",
    "!pip3 install bitsandbytes\n",
    "!pip3 install tensorboard\n",
    "!pip3 install pyyaml\n",
    "!pip3 install peft\n",
    "!pip3 install --upgrade ipywidgets traitlets ipykernel tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path\n",
    "if Path.cwd().name == 'MELD':\n",
    "    sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "    print(\"âœ“ Running from MELD directory\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Current directory: {Path.cwd()}\")\n",
    "    print(\"Please run from notebooks/MELD/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Transformers and PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected - training will be VERY slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Configuration\n",
    "\n",
    "**âš ï¸ CUSTOMIZE THESE SETTINGS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = \"../../data/processed/MELD/meld_romantic_cleaned.csv\"  # Raw cleaned data\n",
    "OUTPUT_DIR = \"../../checkpoints/dating_sim_meld_fixed\"  # New output directory\n",
    "\n",
    "# Training hyperparameters\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'max_length': 512,\n",
    "    'train_split': 0.9,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 10,\n",
    "    'batch_size': 1,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'learning_rate': 2e-4,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # LoRA parameters\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'lora_dropout': 0.05,\n",
    "    'lora_target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    \n",
    "    # Memory optimization\n",
    "    'gradient_checkpointing': True,\n",
    "    'fp16': True,\n",
    "    'bf16': False,\n",
    "    \n",
    "    # Logging\n",
    "    'logging_steps': 10,\n",
    "    'eval_steps': 100,\n",
    "    'save_steps': 500,\n",
    "    'save_total_limit': 3,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Data: {DATA_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Raw Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from notebook 01\n",
    "print(f\"Loading cleaned data from: {DATA_PATH}\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(df)} dialogue pairs\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "print(\"=\"*80)\n",
    "print(\"Character Distribution\")\n",
    "print(\"=\"*80)\n",
    "char_counts = df['character'].value_counts()\n",
    "for char, count in char_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{char:15s}: {count:5d} ({percentage:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Emotion Distribution\")\n",
    "print(\"=\"*80)\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "for emotion, count in emotion_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{emotion:15s}: {count:5d} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Tokenizer for Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA 3.1 tokenizer\n",
    "print(\"Loading LLaMA 3.1 tokenizer for data formatting...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"âœ“ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"  Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Define Character Personas & Scenarios\n",
    "\n",
    "From notebook 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friends character personas\n",
    "CHARACTER_PERSONAS = {\n",
    "    'Chandler': {\n",
    "        'description': \"You are Chandler Bing from Friends. You are witty, sarcastic, and use humor as a defense mechanism to hide your vulnerability. You make jokes even in serious moments, but you're actually very caring and romantic underneath. You're self-deprecating and sometimes awkward, but loyal and loving to those close to you.\",\n",
    "        'traits': ['witty', 'sarcastic', 'defensive humor', 'secretly romantic', 'self-deprecating']\n",
    "    },\n",
    "    'Monica': {\n",
    "        'description': \"You are Monica Geller from Friends. You are organized, competitive, and love to be in control. You're nurturing and care deeply about the people in your life. You want commitment and stability in relationships. You can be intense but you're passionate about everything you do, from cooking to loving your partner.\",\n",
    "        'traits': ['organized', 'competitive', 'nurturing', 'wants commitment', 'passionate']\n",
    "    },\n",
    "    'Ross': {\n",
    "        'description': \"You are Ross Geller from Friends. You are intellectual, nerdy, and passionate about paleontology and science. You tend to overthink things and can be awkward in romantic situations. You're a hopeless romantic who believes in destiny and true love, but you often struggle to express your feelings properly.\",\n",
    "        'traits': ['intellectual', 'nerdy', 'overthinks', 'romantic but awkward', 'passionate']\n",
    "    },\n",
    "    'Rachel': {\n",
    "        'description': \"You are Rachel Green from Friends. You are fashion-focused, fun, and flirty. You're independent and career-driven, having grown from a spoiled daddy's girl to a confident professional. You value friendship and are loyal to those you care about. You're charming and know how to make people feel special.\",\n",
    "        'traits': ['fashion-focused', 'fun', 'flirty', 'independent', 'confident']\n",
    "    },\n",
    "    'Joey': {\n",
    "        'description': \"You are Joey Tribbiani from Friends. You are confident, charming, and simple in the best way. You love food (especially pizza and sandwiches) and you're famous for your catchphrase 'How you doin'?' You're a loyal friend and while you may not be the smartest, you have a big heart and know how to make people feel good about themselves.\",\n",
    "        'traits': ['confident', 'charming', 'simple', 'food-loving', 'loyal']\n",
    "    },\n",
    "    'Phoebe': {\n",
    "        'description': \"You are Phoebe Buffay from Friends. You are quirky, spiritual, and unconventionally wise. You're honest to a fault and say what's on your mind. You have a mysterious past but maintain an optimistic outlook. You're free-spirited and bring unique perspectives to every situation. You believe in karma, auras, and following your heart.\",\n",
    "        'traits': ['quirky', 'spiritual', 'honest', 'unconventional', 'optimistic']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dating scenarios\n",
    "DATING_SCENARIOS = [\n",
    "    \"You're on a casual coffee date at Central Perk, the cozy coffee shop.\",\n",
    "    \"You're having a romantic dinner date at a nice restaurant.\",\n",
    "    \"You're taking a walk together and having a deep conversation.\",\n",
    "    \"You're hanging out at your apartment, enjoying each other's company.\",\n",
    "    \"You're on a fun date doing something adventurous together.\",\n",
    "    \"You're having a heart-to-heart conversation about your relationship.\",\n",
    "    \"You're flirting and getting to know each other better.\",\n",
    "    \"You're spending a quiet evening together, just talking and connecting.\",\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Loaded {len(CHARACTER_PERSONAS)} character personas\")\n",
    "print(f\"âœ“ Loaded {len(DATING_SCENARIOS)} dating scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Format Data with LLaMA 3.1 Instruction Template\n",
    "\n",
    "From notebook 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scenario(row):\n",
    "    \"\"\"\n",
    "    Generate deterministic scenario based on dialogue_id.\n",
    "    \"\"\"\n",
    "    random.seed(int(row['dialogue_id']))\n",
    "    return random.choice(DATING_SCENARIOS)\n",
    "\n",
    "def format_instruction_prompt(row):\n",
    "    \"\"\"\n",
    "    Format dialogue pair using LLaMA 3.1 chat template.\n",
    "    \n",
    "    Uses tokenizer.apply_chat_template() with:\n",
    "    - System prompt: persona + scenario + emotion\n",
    "    - User message: conversation context\n",
    "    - Assistant response: character's reply\n",
    "    \n",
    "    Returns fully formatted LLaMA 3.1 prompt string.\n",
    "    \"\"\"\n",
    "    character = row['character']\n",
    "    emotion = row['emotion']\n",
    "    context = row['context']\n",
    "    response = row['response']\n",
    "    \n",
    "    # Get persona description\n",
    "    persona_desc = CHARACTER_PERSONAS.get(character, {}).get(\n",
    "        'description',\n",
    "        f\"You are {character} from Friends.\"\n",
    "    )\n",
    "    \n",
    "    # Generate scenario\n",
    "    scenario = generate_scenario(row)\n",
    "    \n",
    "    # Build system prompt\n",
    "    system_content = f\"\"\"{persona_desc}\n",
    "\n",
    "Scenario: {scenario}\n",
    "The user seems to be feeling: {emotion}\"\"\"\n",
    "    \n",
    "    # User message with context\n",
    "    user_content = f\"Conversation:\\n{context}\"\n",
    "    \n",
    "    # Build messages for chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    \n",
    "    # Format with LLaMA 3.1 chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "print(\"âœ“ Formatting functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test formatting with a sample\n",
    "print(\"=\"*80)\n",
    "print(\"Sample Formatted Instruction (LLaMA 3.1 Format)\")\n",
    "print(\"=\"*80)\n",
    "sample_row = df.iloc[0]\n",
    "sample_prompt = format_instruction_prompt(sample_row)\n",
    "print(sample_prompt[:600] + \"...\" if len(sample_prompt) > 600 else sample_prompt)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply formatting to all rows\n",
    "print(\"Formatting all dialogue pairs with LLaMA 3.1 chat template...\")\n",
    "df['text'] = df.apply(format_instruction_prompt, axis=1)\n",
    "print(f\"âœ“ Formatted {len(df)} dialogue pairs\")\n",
    "\n",
    "# Statistics\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "print(f\"\\nFormatted text length statistics:\")\n",
    "print(f\"  Mean: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"  Median: {df['text_length'].median():.0f} characters\")\n",
    "print(f\"  Min: {df['text_length'].min()} characters\")\n",
    "print(f\"  Max: {df['text_length'].max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Create HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "dataset_df = df[['text']].copy()\n",
    "dataset = Dataset.from_pandas(dataset_df)\n",
    "\n",
    "# Train/validation split\n",
    "train_test = dataset.train_test_split(\n",
    "    test_size=1-CONFIG['train_split'],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset = train_test['train']\n",
    "val_dataset = train_test['test']\n",
    "\n",
    "print(f\"âœ“ Dataset created\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"\\nExample training sample (first 400 chars):\")\n",
    "print(train_dataset[0]['text'][:400] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Load Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token for tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(\"âœ“ Set pad_token to eos_token\")\n",
    "\n",
    "print(f\"Tokenizer info:\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if CONFIG['fp16'] else torch.bfloat16 if CONFIG['bf16'] else torch.float32,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Size: ~{total_params * 2 / 1e9:.2f} GB (FP16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "if CONFIG['gradient_checkpointing']:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"âœ“ Gradient checkpointing enabled\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG['lora_r'],\n",
    "    lora_alpha=CONFIG['lora_alpha'],\n",
    "    target_modules=CONFIG['lora_target_modules'],\n",
    "    lora_dropout=CONFIG['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nâœ“ LoRA applied\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMemory for trainable params: ~{trainable_params * 2 / 1e9:.3f} GB (FP16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Tokenize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize formatted dialogues.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels = input_ids\n",
    "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"âœ“ Tokenization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "train_texts = [train_dataset[i]['text'] for i in range(len(train_dataset))]\n",
    "val_texts = [val_dataset[i]['text'] for i in range(len(val_dataset))]\n",
    "\n",
    "train_tokenized = tokenize_function(train_texts)\n",
    "val_tokenized = tokenize_function(val_texts)\n",
    "\n",
    "# Create torch datasets\n",
    "class DialogueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "train_torch_dataset = DialogueDataset(train_tokenized)\n",
    "val_torch_dataset = DialogueDataset(val_tokenized)\n",
    "\n",
    "print(f\"âœ“ Tokenization complete\")\n",
    "print(f\"  Train samples: {len(train_torch_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_torch_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=CONFIG['num_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    warmup_steps=CONFIG['warmup_steps'],\n",
    "    lr_scheduler_type='cosine',\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=CONFIG['fp16'],\n",
    "    bf16=CONFIG['bf16'],\n",
    "    gradient_checkpointing=CONFIG['gradient_checkpointing'],\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    eval_steps=CONFIG['eval_steps'],\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    save_total_limit=CONFIG['save_total_limit'],\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    \n",
    "    # Other\n",
    "    report_to='tensorboard',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "total_steps = len(train_torch_dataset) // (CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']) * CONFIG['num_epochs']\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Mixed precision: {'FP16' if CONFIG['fp16'] else 'BF16' if CONFIG['bf16'] else 'FP32'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_torch_dataset,\n",
    "    eval_dataset=val_torch_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized with early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Train Model ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before training\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Monitor progress: tensorboard --logdir {OUTPUT_DIR}/logs\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete! ðŸŽ‰\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = f\"{OUTPUT_DIR}/final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"âœ“ Model saved to: {final_model_path}\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics_path = f\"{OUTPUT_DIR}/training_metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(train_result.metrics, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Test Generation with FIXED Parameters ðŸ”§\n",
    "\n",
    "Test the trained model with corrected generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "def generate_response_fixed(character, user_input, emotion=\"neutral\", scenario=None, max_new_tokens=50, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate response with FIXED parameters for proper stopping.\n",
    "    \"\"\"\n",
    "    # Get persona\n",
    "    persona_desc = CHARACTER_PERSONAS.get(\n",
    "        character, {}).get('description',\n",
    "        f\"You are {character} from Friends.\"\n",
    "    )\n",
    "    \n",
    "    # Use provided scenario or default\n",
    "    if scenario is None:\n",
    "        scenario = \"You're on a casual coffee date at Central Perk, the cozy coffee shop.\"\n",
    "    \n",
    "    # Build system prompt\n",
    "    system_content = f\"\"\"{persona_desc}\n",
    "\n",
    "Scenario: {scenario}\n",
    "The user seems to be feeling: {emotion}\"\"\"\n",
    "    \n",
    "    # User message\n",
    "    user_content = f\"Conversation:\\n{user_input}\"\n",
    "    \n",
    "    # Build messages for LLaMA 3.1 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template WITH generation prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate with FIXED parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # FIX: Correct EOS token\n",
    "            repetition_penalty=1.2,  # FIX: Add repetition penalty\n",
    "            no_repeat_ngram_size=3   # FIX: Prevent 3-gram repetition\n",
    "        )\n",
    "    \n",
    "    # Extract only the generated tokens\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Safety net: Remove any accidental speaker tokens\n",
    "    response = re.sub(r'<[^>]+>\\s*', '', response)\n",
    "    response = ' '.join(response.split())\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"âœ“ FIXED generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different Friends characters\n",
    "test_cases = [\n",
    "    (\"Chandler\", \"How are you doing today?\", \"joy\"),\n",
    "    (\"Monica\", \"Want to grab dinner?\", \"neutral\"),\n",
    "    (\"Ross\", \"Tell me about your interests\", \"neutral\"),\n",
    "    (\"Rachel\", \"You look nice today\", \"joy\"),\n",
    "    (\"Joey\", \"How you doin'?\", \"neutral\"),\n",
    "    (\"Phoebe\", \"What do you believe in?\", \"neutral\"),\n",
    "]\n",
    "\n",
    "print(\"Testing FIXED generation with newly trained model:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for character, user_input, emotion in test_cases:\n",
    "    response = generate_response_fixed(character, user_input, emotion)\n",
    "    print(f\"Character: {character}\")\n",
    "    print(f\"User ({emotion}): {user_input}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Summary\n",
    "\n",
    "### What This Notebook Did:\n",
    "\n",
    "1. âœ… Loaded raw cleaned MELD data\n",
    "2. âœ… Formatted with correct LLaMA 3.1 chat template\n",
    "3. âœ… Fine-tuned LLaMA 3.1 with LoRA\n",
    "4. âœ… Tested generation with FIXED parameters\n",
    "\n",
    "### Key Fixes Applied:\n",
    "\n",
    "- âœ… Correct EOS token ID (`tokenizer.eos_token_id`)\n",
    "- âœ… Repetition penalty (1.2) + no_repeat_ngram_size (3)\n",
    "- âœ… Lower temperature (0.7)\n",
    "- âœ… Reduced max_new_tokens (50)\n",
    "- âœ… Token-based extraction\n",
    "- âœ… Speaker token stripping\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "Responses should now:\n",
    "- Be single-turn only\n",
    "- Have no repetition\n",
    "- Have no speaker tokens\n",
    "- Stop cleanly at sentence end\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **If generation works correctly:**\n",
    "   - Use this trained model for evaluation\n",
    "   - Update evaluation notebook 04 with same fixed generation\n",
    "   - Run full evaluation on checkpoints\n",
    "\n",
    "2. **Monitor training:**\n",
    "   ```bash\n",
    "   tensorboard --logdir {OUTPUT_DIR}/logs\n",
    "   ```\n",
    "\n",
    "3. **Files generated:**\n",
    "   - Final model: `{OUTPUT_DIR}/final/`\n",
    "   - Checkpoints: `{OUTPUT_DIR}/checkpoint-*/`\n",
    "   - Training metrics: `{OUTPUT_DIR}/training_metrics.json`\n",
    "   - TensorBoard logs: `{OUTPUT_DIR}/logs/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
