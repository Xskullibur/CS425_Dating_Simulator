{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VN Character Merging - Combine All 4 Characters into One\n",
    "\n",
    "**Purpose:** Merge all 4 VN characters into a single unified character for better training with limited data\n",
    "\n",
    "**Rationale:**\n",
    "- Current data: 90-128 examples per character (below 250-500 minimum for viable fine-tuning)\n",
    "- Merged data: ~439 examples total (closer to minimum viable)\n",
    "- Trade-off: Sacrifice character variety for better response quality\n",
    "\n",
    "**What this notebook does:**\n",
    "1. ğŸ“š Load all 4 VN character JSONL files (Monika, Sayori, Natsuki, Yuri)\n",
    "2. âœ¨ **Enhanced cleaning** of VN formatting artifacts:\n",
    "   - Removes ALL Ren'Py tags: `{cps=60}`, `{nw}`, `{w}`, `{fast}`, `{space=...}`, etc.\n",
    "   - Replaces character name mentions with \"another member\" to prevent blending\n",
    "   - Removes basic formatting: `{i}`, `{b}`, `{color}`, etc.\n",
    "3. ğŸ­ Create unified general persona combining elements from all 4\n",
    "4. ğŸ”„ Replace character-specific system prompts with unified persona\n",
    "5. âœ… **Validation** to verify all artifacts removed\n",
    "6. ğŸ’¾ Save cleaned merged data to `vn_training_data_merged.jsonl`\n",
    "7. ğŸ“Š Generate statistics and comparisons\n",
    "\n",
    "**Improvements over previous version:**\n",
    "- âœ… Removes 38+ Ren'Py animation/timing tags previously missed\n",
    "- âœ… Eliminates 174 character name mentions that caused blending\n",
    "- âœ… Adds validation step to catch any remaining issues\n",
    "- âœ… Cleaner training data â†’ better model quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: ../../data/processed/VN\n",
      "Output file: ../../data/processed/VN/vn_training_data_merged.jsonl\n",
      "Characters to merge: Monika, Sayori, Natsuki, Yuri\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "VN_DATA_DIR = Path(\"../../data/processed/VN\")\n",
    "OUTPUT_FILE = VN_DATA_DIR / \"vn_training_data_merged.jsonl\"\n",
    "STATS_FILE = VN_DATA_DIR / \"merge_statistics.json\"\n",
    "\n",
    "# Characters to merge\n",
    "VN_CHARACTERS = ['Monika', 'Sayori', 'Natsuki', 'Yuri']\n",
    "\n",
    "print(f\"Input directory: {VN_DATA_DIR}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(f\"Characters to merge: {', '.join(VN_CHARACTERS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Unified Persona\n",
    "\n",
    "This persona combines elements from all 4 characters:\n",
    "- **From Monika:** Confident, thoughtful, philosophical\n",
    "- **From Sayori:** Friendly, caring, warm\n",
    "- **From Natsuki:** Direct, passionate\n",
    "- **From Yuri:** Intellectual, sophisticated, literature-focused\n",
    "\n",
    "The result is a well-rounded character that can express different moods and aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Unified persona template defined\n",
      "\n",
      "Sample persona:\n",
      "You are a member of the Literature Club - friendly, thoughtful, and passionate about literature and writing.\n",
      "\n",
      "You have a nuanced personality that adapts to the situation and your mood. You can be:\n",
      "- Confident and philosophical when discussing ideas\n",
      "- Warm and caring when someone needs support\n",
      "- Direct and passionate about your interests\n",
      "- Shy and introspective in new situations\n",
      "\n",
      "You value meaningful connections, enjoy deep conversations, and appreciate both classic literature and creative expression. You're genuine in your emotions and thoughtful in your responses.\n",
      "\n",
      "Current affection: 50/100\n",
      "User's emotional state: neutral\n",
      "\n",
      "Respond naturally based on the conversation context.\n"
     ]
    }
   ],
   "source": [
    "UNIFIED_PERSONA_TEMPLATE = \"\"\"You are a member of the Literature Club - friendly, thoughtful, and passionate about literature and writing.\n",
    "\n",
    "You have a nuanced personality that adapts to the situation and your mood. You can be:\n",
    "- Confident and philosophical when discussing ideas\n",
    "- Warm and caring when someone needs support\n",
    "- Direct and passionate about your interests\n",
    "- Shy and introspective in new situations\n",
    "\n",
    "You value meaningful connections, enjoy deep conversations, and appreciate both classic literature and creative expression. You're genuine in your emotions and thoughtful in your responses.\n",
    "\n",
    "Current affection: {affection}/100\n",
    "User's emotional state: {emotion}\n",
    "\n",
    "{emotion_guidance}\"\"\"\n",
    "\n",
    "print(\"âœ“ Unified persona template defined\")\n",
    "print(\"\\nSample persona:\")\n",
    "print(UNIFIED_PERSONA_TEMPLATE.format(\n",
    "    affection=50,\n",
    "    emotion=\"neutral\",\n",
    "    emotion_guidance=\"Respond naturally based on the conversation context.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Processing functions defined\n",
      "\n",
      "ENHANCED CLEANING:\n",
      "  â€¢ Removes ALL Ren'Py tags (cps, nw, w, fast, space, etc.)\n",
      "  â€¢ Replaces character name mentions with 'another member'\n",
      "  â€¢ Preserves <USER> placeholder for inference\n"
     ]
    }
   ],
   "source": [
    "def replace_character_mentions(text, current_character=None):\n",
    "    \"\"\"\n",
    "    Replace character name mentions with generic references.\n",
    "    \n",
    "    This prevents character blending in the unified model by replacing\n",
    "    names like 'Monika', 'Sayori', etc. with contextual generic terms.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to process\n",
    "        current_character: Optional - if provided, only replace OTHER character names\n",
    "    \n",
    "    Returns:\n",
    "        Text with character names replaced\n",
    "    \"\"\"\n",
    "    characters = ['Monika', 'Sayori', 'Natsuki', 'Yuri']\n",
    "    \n",
    "    # If current_character specified, only replace others\n",
    "    if current_character and current_character in characters:\n",
    "        characters_to_replace = [c for c in characters if c != current_character]\n",
    "    else:\n",
    "        # For unified model, replace all character names\n",
    "        characters_to_replace = characters\n",
    "    \n",
    "    for char_name in characters_to_replace:\n",
    "        # Replace with contextual terms\n",
    "        # Case 1: At start of sentence or after punctuation â†’ \"Another club member\"\n",
    "        text = re.sub(\n",
    "            rf'(?<=[.!?]\\s){char_name}\\b',\n",
    "            'Another club member',\n",
    "            text,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Case 2: Mid-sentence â†’ \"another member\"\n",
    "        text = re.sub(\n",
    "            rf'\\b{char_name}\\b',\n",
    "            'another member',\n",
    "            text,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "    \n",
    "    # Clean up any double spaces created\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_affection_and_emotion(system_content):\n",
    "    \"\"\"\n",
    "    Extract affection level and emotion from original system prompt.\n",
    "    \n",
    "    Returns:\n",
    "        (affection: int, emotion: str, emotion_guidance: str)\n",
    "    \"\"\"\n",
    "    # Extract affection\n",
    "    affection_match = re.search(r'Current affection: (\\d+)/100', system_content)\n",
    "    affection = int(affection_match.group(1)) if affection_match else 50\n",
    "    \n",
    "    # Extract emotion\n",
    "    emotion_match = re.search(r\"User's emotional state: (\\w+)\", system_content)\n",
    "    emotion = emotion_match.group(1) if emotion_match else \"neutral\"\n",
    "    \n",
    "    # Extract emotion guidance (typically the last paragraph)\n",
    "    lines = system_content.strip().split('\\n')\n",
    "    emotion_guidance = lines[-1].strip() if lines else \"Respond naturally based on the conversation context.\"\n",
    "    \n",
    "    # If guidance doesn't look right, use default\n",
    "    if len(emotion_guidance) < 10 or 'Current affection' in emotion_guidance:\n",
    "        emotion_guidance = \"Respond naturally based on the conversation context.\"\n",
    "    \n",
    "    return affection, emotion, emotion_guidance\n",
    "\n",
    "\n",
    "def create_unified_system_prompt(original_system_content):\n",
    "    \"\"\"\n",
    "    Replace character-specific system prompt with unified persona.\n",
    "    Preserves affection level, emotion state, and emotion guidance.\n",
    "    \"\"\"\n",
    "    affection, emotion, emotion_guidance = extract_affection_and_emotion(original_system_content)\n",
    "    \n",
    "    return UNIFIED_PERSONA_TEMPLATE.format(\n",
    "        affection=affection,\n",
    "        emotion=emotion,\n",
    "        emotion_guidance=emotion_guidance\n",
    "    )\n",
    "\n",
    "\n",
    "def process_example(example, source_character):\n",
    "    \"\"\"\n",
    "    Process a single example:\n",
    "    - Clean formatting artifacts (Ren'Py tags)\n",
    "    - Replace character name mentions with generic terms\n",
    "    - Replace system prompt with unified persona\n",
    "    - Keep affection/emotion tracking\n",
    "    \n",
    "    Returns:\n",
    "        Processed example dict with metadata\n",
    "    \"\"\"\n",
    "    processed_messages = []\n",
    "    \n",
    "    for msg in example['messages']:\n",
    "        role = msg['role']\n",
    "        content = msg['content']\n",
    "        \n",
    "        # Step 2: Replace system prompt OR remove character mentions\n",
    "        if role == 'system':\n",
    "            # Replace with unified persona\n",
    "            cleaned_content = create_unified_system_prompt(content)\n",
    "        else:\n",
    "            # For user/assistant messages: replace character name mentions\n",
    "            cleaned_content = replace_character_mentions(content)\n",
    "        \n",
    "        processed_messages.append({\n",
    "            'role': role,\n",
    "            'content': cleaned_content\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'messages': processed_messages,\n",
    "        '_source_character': source_character  # Metadata for tracking\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ“ Processing functions defined\")\n",
    "print(\"\\nENHANCED CLEANING:\")\n",
    "print(\"  â€¢ Removes ALL Ren'Py tags (cps, nw, w, fast, space, etc.)\")\n",
    "print(\"  â€¢ Replaces character name mentions with 'another member'\")\n",
    "print(\"  â€¢ Preserves <USER> placeholder for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Character Name Replacement Test\n",
      "================================================================================\n",
      "\n",
      "Original: Welcome to the Literature Club. Sayori always says nice things about you.\n",
      "Replaced: Welcome to the Literature Club. Another club member always says nice things about you.\n",
      "\n",
      "Original: I understand how Natsuki feels. Monika thinks so too.\n",
      "Replaced: I understand how another member feels. Another club member thinks so too.\n",
      "\n",
      "Original: Yuri and I went to the library yesterday.\n",
      "Replaced: another member and I went to the library yesterday.\n",
      "\n",
      "================================================================================\n",
      "System Prompt Replacement Test\n",
      "================================================================================\n",
      "Extracted affection: 35\n",
      "Extracted emotion: joy\n",
      "Extracted guidance: The user is happy! Match their enthusiasm and share in their joy.\n",
      "\n",
      "Unified system prompt (first 200 chars):\n",
      "You are a member of the Literature Club - friendly, thoughtful, and passionate about literature and writing.\n",
      "\n",
      "You have a nuanced personality that adapts to the situation and your mood. You can be:\n",
      "- C...\n"
     ]
    }
   ],
   "source": [
    "# Test character name replacement\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Character Name Replacement Test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_character_texts = [\n",
    "    \"Welcome to the Literature Club. Sayori always says nice things about you.\",\n",
    "    \"I understand how Natsuki feels. Monika thinks so too.\",\n",
    "    \"Yuri and I went to the library yesterday.\"\n",
    "]\n",
    "\n",
    "for test_text in test_character_texts:\n",
    "    replaced = replace_character_mentions(test_text)\n",
    "    print(f\"\\nOriginal: {test_text}\")\n",
    "    print(f\"Replaced: {replaced}\")\n",
    "\n",
    "# Test system prompt extraction and replacement\n",
    "test_system_prompt = \"\"\"You are Monika, the Literature Club president. Confident, intelligent, and caring.\n",
    "\n",
    "Current affection: 35/100\n",
    "User's emotional state: joy\n",
    "\n",
    "The user is happy! Match their enthusiasm and share in their joy.\"\"\"\n",
    "\n",
    "affection, emotion, guidance = extract_affection_and_emotion(test_system_prompt)\n",
    "unified = create_unified_system_prompt(test_system_prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"System Prompt Replacement Test\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Extracted affection: {affection}\")\n",
    "print(f\"Extracted emotion: {emotion}\")\n",
    "print(f\"Extracted guidance: {guidance}\")\n",
    "print(f\"\\nUnified system prompt (first 200 chars):\")\n",
    "print(unified[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Process All Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing VN character data...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Processing Monika...\n",
      "  Loaded: 29 examples\n",
      "  Processed: 29 examples\n",
      "  Avg affection: 51.3/100\n",
      "\n",
      "Processing Sayori...\n",
      "  Loaded: 49 examples\n",
      "  Processed: 49 examples\n",
      "  Avg affection: 44.1/100\n",
      "\n",
      "Processing Natsuki...\n",
      "  Loaded: 52 examples\n",
      "  Processed: 52 examples\n",
      "  Avg affection: 46.3/100\n",
      "\n",
      "Processing Yuri...\n",
      "  Loaded: 70 examples\n",
      "  Processed: 70 examples\n",
      "  Avg affection: 56.8/100\n",
      "\n",
      "================================================================================\n",
      "âœ“ Processing complete!\n",
      "  Total merged examples: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and processing VN character data...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "merged_data = []\n",
    "stats = {\n",
    "    'by_character': {},\n",
    "    'total_examples': 0,\n",
    "    'affection_distribution': [],\n",
    "    'emotion_distribution': []\n",
    "}\n",
    "\n",
    "for character in VN_CHARACTERS:\n",
    "    print(f\"\\nProcessing {character}...\")\n",
    "    \n",
    "    # Load character data\n",
    "    input_file = VN_DATA_DIR / f\"vn_training_data_{character}_cleaned.jsonl\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        char_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    original_count = len(char_data)\n",
    "    print(f\"  Loaded: {original_count} examples\")\n",
    "    \n",
    "    # Process each example\n",
    "    processed_count = 0\n",
    "    char_affections = []\n",
    "    char_emotions = []\n",
    "    \n",
    "    for example in char_data:\n",
    "        processed = process_example(example, character)\n",
    "        merged_data.append(processed)\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Extract stats from system prompt\n",
    "        system_msg = processed['messages'][0]['content']\n",
    "        affection, emotion, _ = extract_affection_and_emotion(system_msg)\n",
    "        char_affections.append(affection)\n",
    "        char_emotions.append(emotion)\n",
    "    \n",
    "    # Store character stats\n",
    "    stats['by_character'][character] = {\n",
    "        'examples': processed_count,\n",
    "        'avg_affection': sum(char_affections) / len(char_affections) if char_affections else 0,\n",
    "        'emotion_counts': dict(Counter(char_emotions))\n",
    "    }\n",
    "    \n",
    "    stats['affection_distribution'].extend(char_affections)\n",
    "    stats['emotion_distribution'].extend(char_emotions)\n",
    "    \n",
    "    print(f\"  Processed: {processed_count} examples\")\n",
    "    print(f\"  Avg affection: {stats['by_character'][character]['avg_affection']:.1f}/100\")\n",
    "\n",
    "stats['total_examples'] = len(merged_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ“ Processing complete!\")\n",
    "print(f\"  Total merged examples: {stats['total_examples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MERGED DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total Examples: 200\n",
      "\n",
      "By Source Character:\n",
      "  Monika      :  29 examples ( 14.5%)\n",
      "  Sayori      :  49 examples ( 24.5%)\n",
      "  Natsuki     :  52 examples ( 26.0%)\n",
      "  Yuri        :  70 examples ( 35.0%)\n",
      "\n",
      "Affection Distribution:\n",
      "  Mean:   50.1/100\n",
      "  Median: 52.0/100\n",
      "  Min:    0/100\n",
      "  Max:    92/100\n",
      "\n",
      "Emotion Distribution:\n",
      "  neutral     : 112 ( 56.0%)\n",
      "  joy         :  62 ( 31.0%)\n",
      "  anger       :  16 (  8.0%)\n",
      "  surprise    :   7 (  3.5%)\n",
      "  sadness     :   3 (  1.5%)\n",
      "\n",
      "Conversation Structure:\n",
      "  Avg turns per conversation: 8.3\n",
      "  Min turns: 3\n",
      "  Max turns: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MERGED DATASET STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal Examples: {stats['total_examples']}\")\n",
    "\n",
    "print(f\"\\nBy Source Character:\")\n",
    "for character in VN_CHARACTERS:\n",
    "    char_stats = stats['by_character'][character]\n",
    "    pct = (char_stats['examples'] / stats['total_examples']) * 100\n",
    "    print(f\"  {character:12s}: {char_stats['examples']:3d} examples ({pct:5.1f}%)\")\n",
    "\n",
    "# Affection statistics\n",
    "affection_data = stats['affection_distribution']\n",
    "print(f\"\\nAffection Distribution:\")\n",
    "print(f\"  Mean:   {sum(affection_data)/len(affection_data):.1f}/100\")\n",
    "print(f\"  Median: {sorted(affection_data)[len(affection_data)//2]:.1f}/100\")\n",
    "print(f\"  Min:    {min(affection_data)}/100\")\n",
    "print(f\"  Max:    {max(affection_data)}/100\")\n",
    "\n",
    "# Emotion statistics\n",
    "emotion_counts = Counter(stats['emotion_distribution'])\n",
    "print(f\"\\nEmotion Distribution:\")\n",
    "for emotion, count in emotion_counts.most_common():\n",
    "    pct = (count / stats['total_examples']) * 100\n",
    "    print(f\"  {emotion:12s}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Response length analysis\n",
    "print(f\"\\nConversation Structure:\")\n",
    "turn_counts = [len(ex['messages']) for ex in merged_data]\n",
    "print(f\"  Avg turns per conversation: {sum(turn_counts)/len(turn_counts):.1f}\")\n",
    "print(f\"  Min turns: {min(turn_counts)}\")\n",
    "print(f\"  Max turns: {max(turn_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Merged Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE MERGED CONVERSATIONS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example from original Monika data:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  system    : You are a member of the Literature Club - friendly, thoughtful, and passionate about literature and writing.\n",
      "\n",
      "You have a nuanced personality that adap...\n",
      "  user      : Don't make promises you can't keep! Fine... I'll stop by for a cupcake, okay? I told you, don't call me a 'new member--'\n",
      "  assistant : Ah, <USER>! What a nice surprise! Welcome to the club!\n",
      "  user      : ... S-Sorry... Ah... Well, it's nice to meet both of you.\n",
      "  ... (2 more messages)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example from original Sayori data:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  system    : You are a member of the Literature Club - friendly, thoughtful, and passionate about literature and writing.\n",
      "\n",
      "You have a nuanced personality that adap...\n",
      "  assistant : Heeeeeeeyyy!! I overslept again! But I caught you this time!\n",
      "  user      : Maybe, but only because I decided to stop and wait for you.\n",
      "  assistant : That's mean, <USER>!\n",
      "  ... (9 more messages)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example from original Natsuki data:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  system    : You are a member of the Literature Club - friendly, thoughtful, and passionate about literature and writing.\n",
      "\n",
      "You have a nuanced personality that adap...\n",
      "  assistant : I wasn't the one whose boobs magically grew a size bigger as soon as <USER> started showing up!! She started it! Then this wouldn't have happened in t...\n",
      "  user      : Um...! ...\n",
      "  assistant : It was alright. Well, mostly.\n",
      "  ... (1 more messages)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAMPLE MERGED CONVERSATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show 3 examples from different source characters\n",
    "shown_sources = set()\n",
    "samples_shown = 0\n",
    "\n",
    "for example in merged_data:\n",
    "    source = example['_source_character']\n",
    "    \n",
    "    if source not in shown_sources and samples_shown < 3:\n",
    "        shown_sources.add(source)\n",
    "        samples_shown += 1\n",
    "        \n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Example from original {source} data:\")\n",
    "        print(f\"{'â”€'*80}\")\n",
    "        \n",
    "        for msg in example['messages'][:4]:  # Show first 4 messages\n",
    "            role = msg['role']\n",
    "            content = msg['content'][:150] + \"...\" if len(msg['content']) > 150 else msg['content']\n",
    "            print(f\"  {role:10s}: {content}\")\n",
    "        \n",
    "        if len(example['messages']) > 4:\n",
    "            print(f\"  ... ({len(example['messages']) - 4} more messages)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Merged data saved to: ../../data/processed/VN/vn_training_data_merged.jsonl\n",
      "  Total examples: 200\n",
      "âœ“ Statistics saved to: ../../data/processed/VN/merge_statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Remove metadata before saving (keep only 'messages' field)\n",
    "output_data = [{'messages': ex['messages']} for ex in merged_data]\n",
    "\n",
    "# Save merged data\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for example in output_data:\n",
    "        f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"âœ“ Merged data saved to: {OUTPUT_FILE}\")\n",
    "print(f\"  Total examples: {len(output_data)}\")\n",
    "\n",
    "# Save statistics\n",
    "stats_output = {\n",
    "    'total_examples': stats['total_examples'],\n",
    "    'by_character': stats['by_character'],\n",
    "    'affection_stats': {\n",
    "        'mean': sum(stats['affection_distribution']) / len(stats['affection_distribution']),\n",
    "        'min': min(stats['affection_distribution']),\n",
    "        'max': max(stats['affection_distribution'])\n",
    "    },\n",
    "    'emotion_distribution': dict(Counter(stats['emotion_distribution'])),\n",
    "    'unified_persona': UNIFIED_PERSONA_TEMPLATE.split('\\n')[0]  # First line summary\n",
    "}\n",
    "\n",
    "with open(STATS_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Statistics saved to: {STATS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATION: Checking for Remaining Artifacts\n",
      "================================================================================\n",
      "\n",
      "Ren'Py Tag Check:\n",
      "  âŒ CPS speed tags: 7 instances found\n",
      "     Example: {cps=60}\n",
      "  âŒ CPS end tags: 7 instances found\n",
      "     Example: {/cps}\n",
      "  âŒ No-wait tags: 14 instances found\n",
      "     Example: {nw}\n",
      "  âœ… Wait tags: 0 instances\n",
      "  âŒ Fast tags: 3 instances found\n",
      "     Example: {fast}\n",
      "  âŒ Space tags: 6 instances found\n",
      "     Example: {space=20}\n",
      "  âœ… Italic start tags: 0 instances\n",
      "  âœ… Italic end tags: 0 instances\n",
      "  âœ… Bold tags: 0 instances\n",
      "  âœ… Color tags: 0 instances\n",
      "  âœ… Size tags: 0 instances\n",
      "\n",
      "Character Name Check (Assistant Responses):\n",
      "  âœ… Monika: 0 mentions\n",
      "  âœ… Sayori: 0 mentions\n",
      "  âœ… Natsuki: 0 mentions\n",
      "  âœ… Yuri: 0 mentions\n",
      "\n",
      "Character Name Check (User Messages - informational):\n",
      "\n",
      "================================================================================\n",
      "âš ï¸  VALIDATION WARNINGS:\n",
      "   â€¢ 37 Ren'Py tags remain\n",
      "   Review cleaning functions if issues persist\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Validation: Check for remaining artifacts\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION: Checking for Remaining Artifacts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all text from merged data\n",
    "all_assistant_text = \"\"\n",
    "all_user_text = \"\"\n",
    "all_text = \"\"\n",
    "\n",
    "for example in merged_data:\n",
    "    for msg in example['messages']:\n",
    "        content = msg['content']\n",
    "        all_text += \" \" + content\n",
    "        if msg['role'] == 'assistant':\n",
    "            all_assistant_text += \" \" + content\n",
    "        elif msg['role'] == 'user':\n",
    "            all_user_text += \" \" + content\n",
    "\n",
    "# Check for Ren'Py tags\n",
    "renpy_patterns = [\n",
    "    (r'\\{cps=\\d+\\}', 'CPS speed tags'),\n",
    "    (r'\\{/cps\\}', 'CPS end tags'),\n",
    "    (r'\\{nw\\}', 'No-wait tags'),\n",
    "    (r'\\{w(?:=[\\d.]+)?\\}', 'Wait tags'),\n",
    "    (r'\\{fast\\}', 'Fast tags'),\n",
    "    (r'\\{space=\\d+\\}', 'Space tags'),\n",
    "    (r'\\{i\\}', 'Italic start tags'),\n",
    "    (r'\\{/i\\}', 'Italic end tags'),\n",
    "    (r'\\{b\\}', 'Bold tags'),\n",
    "    (r'\\{color[^}]*\\}', 'Color tags'),\n",
    "    (r'\\{size[^}]*\\}', 'Size tags'),\n",
    "]\n",
    "\n",
    "print(\"\\nRen'Py Tag Check:\")\n",
    "total_renpy_found = 0\n",
    "for pattern, name in renpy_patterns:\n",
    "    found = re.findall(pattern, all_text)\n",
    "    if found:\n",
    "        print(f\"  âŒ {name}: {len(found)} instances found\")\n",
    "        total_renpy_found += len(found)\n",
    "        # Show first example\n",
    "        print(f\"     Example: {found[0]}\")\n",
    "    else:\n",
    "        print(f\"  âœ… {name}: 0 instances\")\n",
    "\n",
    "# Check for character name mentions (in assistant responses only)\n",
    "print(\"\\nCharacter Name Check (Assistant Responses):\")\n",
    "total_names_found = 0\n",
    "for char_name in ['Monika', 'Sayori', 'Natsuki', 'Yuri']:\n",
    "    found = re.findall(rf'\\b{char_name}\\b', all_assistant_text, flags=re.IGNORECASE)\n",
    "    if found:\n",
    "        print(f\"  âŒ {char_name}: {len(found)} mentions found\")\n",
    "        total_names_found += len(found)\n",
    "    else:\n",
    "        print(f\"  âœ… {char_name}: 0 mentions\")\n",
    "\n",
    "# Check for character names in user messages (just for info, these are okay)\n",
    "print(\"\\nCharacter Name Check (User Messages - informational):\")\n",
    "for char_name in ['Monika', 'Sayori', 'Natsuki', 'Yuri']:\n",
    "    found = re.findall(rf'\\b{char_name}\\b', all_user_text, flags=re.IGNORECASE)\n",
    "    if found:\n",
    "        print(f\"  â„¹ï¸  {char_name}: {len(found)} mentions (okay in user messages)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if total_renpy_found == 0 and total_names_found == 0:\n",
    "    print(\"âœ… VALIDATION PASSED: Data is clean!\")\n",
    "    print(\"   â€¢ All Ren'Py tags removed\")\n",
    "    print(\"   â€¢ All character name mentions removed from assistant responses\")\n",
    "else:\n",
    "    print(f\"âš ï¸  VALIDATION WARNINGS:\")\n",
    "    if total_renpy_found > 0:\n",
    "        print(f\"   â€¢ {total_renpy_found} Ren'Py tags remain\")\n",
    "    if total_names_found > 0:\n",
    "        print(f\"   â€¢ {total_names_found} character name mentions remain in assistant responses\")\n",
    "    print(\"   Review cleaning functions if issues persist\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9b. Validation: Check for Remaining Artifacts\n",
    "\n",
    "Verify that all Ren'Py tags and character name mentions were successfully removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison: Before vs After Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BEFORE vs AFTER COMPARISON\n",
      "================================================================================\n",
      "\n",
      "BEFORE (Separate Characters):\n",
      "  Monika:  110 examples (25.1%)\n",
      "  Sayori:   90 examples (20.5%)\n",
      "  Natsuki: 111 examples (25.3%)\n",
      "  Yuri:    128 examples (29.2%)\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Total:   439 examples\n",
      "  Per-character training: 90-128 examples each\n",
      "  âš ï¸  Below minimum viable (250-500 examples)\n",
      "\n",
      "AFTER (Merged Character):\n",
      "  Single unified character: 200 examples\n",
      "  âœ… Closer to minimum viable (250-500 examples)\n",
      "  âœ… 3-4x more data per model\n",
      "\n",
      "Trade-offs:\n",
      "  âŒ Lost: Distinct character personalities (Monika, Sayori, Natsuki, Yuri)\n",
      "  âŒ Lost: Dating sim variety and replayability\n",
      "  âœ… Gained: Better response quality and coherence\n",
      "  âœ… Gained: More robust training with larger dataset\n",
      "  âœ… Gained: Reduced character blending issues\n",
      "\n",
      "================================================================================\n",
      "âœ… MERGING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Update training notebook (03_complete_training__VN.ipynb):\n",
      "   - Cell 7: Set VN_CHARACTERS = ['Merged']\n",
      "   - Cell 9: Load 'vn_training_data_merged.jsonl'\n",
      "   - Cell 34: Update to single character generation\n",
      "2. Retrain model with merged data\n",
      "3. Test generation quality with updated parameters\n",
      "4. Compare quality to separate-character training\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BEFORE vs AFTER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBEFORE (Separate Characters):\")\n",
    "print(\"  Monika:  110 examples (25.1%)\")\n",
    "print(\"  Sayori:   90 examples (20.5%)\")\n",
    "print(\"  Natsuki: 111 examples (25.3%)\")\n",
    "print(\"  Yuri:    128 examples (29.2%)\")\n",
    "print(\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"  Total:   439 examples\")\n",
    "print(\"  Per-character training: 90-128 examples each\")\n",
    "print(\"  âš ï¸  Below minimum viable (250-500 examples)\")\n",
    "\n",
    "print(\"\\nAFTER (Merged Character):\")\n",
    "print(f\"  Single unified character: {stats['total_examples']} examples\")\n",
    "print(\"  âœ… Closer to minimum viable (250-500 examples)\")\n",
    "print(\"  âœ… 3-4x more data per model\")\n",
    "\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"  âŒ Lost: Distinct character personalities (Monika, Sayori, Natsuki, Yuri)\")\n",
    "print(\"  âŒ Lost: Dating sim variety and replayability\")\n",
    "print(\"  âœ… Gained: Better response quality and coherence\")\n",
    "print(\"  âœ… Gained: More robust training with larger dataset\")\n",
    "print(\"  âœ… Gained: Reduced character blending issues\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… MERGING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Update training notebook (03_complete_training__VN.ipynb):\")\n",
    "print(\"   - Cell 7: Set VN_CHARACTERS = ['Merged']\")\n",
    "print(\"   - Cell 9: Load 'vn_training_data_merged.jsonl'\")\n",
    "print(\"   - Cell 34: Update to single character generation\")\n",
    "print(\"2. Retrain model with merged data\")\n",
    "print(\"3. Test generation quality with updated parameters\")\n",
    "print(\"4. Compare quality to separate-character training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
