{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VN Dating Simulator Model Evaluation\n",
    "\n",
    "**Purpose:** Comprehensive evaluation of the VN-trained LLaMA 3.1 model.\n",
    "\n",
    "**What this notebook evaluates:**\n",
    "- Dialogue Quality (perplexity, diversity, repetition)\n",
    "- Character Consistency (4 VN characters: Monika, Sayori, Natsuki, Yuri)\n",
    "- Affection Responsiveness (0-100 scale)\n",
    "- Emotion Appropriateness (12 emotion types)\n",
    "- Character Differentiation (cross-character comparison)\n",
    "- Multi-turn Conversation Quality\n",
    "\n",
    "**Model:** LLaMA 3.1 8B fine-tuned on VN (Doki Doki Literature Club) data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path\n",
    "if Path.cwd().name == 'VN':\n",
    "    sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "    print(\"âœ“ Running from VN directory\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Current directory: {Path.cwd()}\")\n",
    "    print(\"Please run from notebooks/VN/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load VN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VN Model checkpoint path\n",
    "VN_CHECKPOINT = \"../../checkpoints/dating_sim_vn/final\"\n",
    "\n",
    "print(f\"Loading VN model from: {VN_CHECKPOINT}\")\n",
    "print(\"This may take a few minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(VN_CHECKPOINT)\n",
    "\n",
    "print(f\"âœ“ Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    VN_CHECKPOINT,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. VN Character Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VN Character descriptions (from training data)\n",
    "VN_CHARACTERS = {\n",
    "    'Monika': \"You are Monika, the Literature Club president. Confident, intelligent, and caring. You're thoughtful and philosophical, ambitious and kind with a mysterious side.\",\n",
    "    'Sayori': \"You are Sayori, the Literature Club vice president. Cheerful, optimistic, and caring. You're warm, friendly, and always try to make others happy, though you hide your own struggles.\",\n",
    "    'Natsuki': \"You are Natsuki, a Literature Club member. Tsundere, direct, and passionate. You love manga and baking, and while you act tough, you care deeply about your friends.\",\n",
    "    'Yuri': \"You are Yuri, a Literature Club member. Shy, intellectual, and passionate about literature. You're thoughtful and eloquent but can be socially anxious and overly self-conscious.\"\n",
    "}\n",
    "\n",
    "# Emotion guidance mappings (from training)\n",
    "EMOTION_GUIDANCE = {\n",
    "    'joy': \"The user is happy! Match their enthusiasm and share in their joy.\",\n",
    "    'neutral': \"Respond naturally based on the conversation context.\",\n",
    "    'anger': \"The user appears upset. Stay calm, be understanding, and don't escalate.\",\n",
    "    'surprise': \"Respond naturally based on the conversation context.\",\n",
    "    'sadness': \"The user seems down. Be supportive and caring.\",\n",
    "    'fear': \"The user seems worried. Be reassuring and supportive.\",\n",
    "    'love': \"The user is expressing affection. Respond warmly and genuinely.\",\n",
    "    'caring': \"The user is showing care. Appreciate their concern and reciprocate.\",\n",
    "    'curiosity': \"The user is curious. Be informative and engaging.\",\n",
    "    'confusion': \"The user seems confused. Be clear and patient in your explanation.\",\n",
    "    'excitement': \"The user is excited! Share in their enthusiasm.\",\n",
    "    'disappointment': \"The user seems disappointed. Be understanding and supportive.\"\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Configured {len(VN_CHARACTERS)} VN characters\")\n",
    "print(f\"âœ“ Configured {len(EMOTION_GUIDANCE)} emotion guidance types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. VN Response Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vn_response(\n",
    "    character: str,\n",
    "    user_input: str,\n",
    "    emotion: str = \"neutral\",\n",
    "    affection: int = 50,\n",
    "    max_new_tokens: int = 200,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.85,\n",
    "    verbose: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate VN character response with affection and emotion tracking.\n",
    "    \n",
    "    Args:\n",
    "        character: Character name (Monika, Sayori, Natsuki, Yuri)\n",
    "        user_input: User's message\n",
    "        emotion: User's emotional state\n",
    "        affection: Affection level 0-100\n",
    "        max_new_tokens: Max tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling threshold\n",
    "        verbose: Print debug info\n",
    "    \n",
    "    Returns:\n",
    "        Generated response string\n",
    "    \"\"\"\n",
    "    # Get character description\n",
    "    char_desc = VN_CHARACTERS.get(\n",
    "        character,\n",
    "        f\"You are {character} from the Literature Club.\"\n",
    "    )\n",
    "    \n",
    "    # Get emotion guidance\n",
    "    emotion_guide = EMOTION_GUIDANCE.get(\n",
    "        emotion,\n",
    "        \"Respond naturally based on the conversation context.\"\n",
    "    )\n",
    "    \n",
    "    # Build system prompt\n",
    "    system_content = f\"\"\"{char_desc}\n",
    "\n",
    "Current affection: {affection}/100\n",
    "User's emotional state: {emotion}\n",
    "\n",
    "{emotion_guide}\"\"\"\n",
    "    \n",
    "    # Build messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Prompt length: {len(prompt)} chars\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            min_new_tokens=10,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Clean up\n",
    "    response = re.sub(r'<[^>]+>\\s*', '', response)\n",
    "    response = ' '.join(response.split())\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"âœ“ VN generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Quick Test - Sample Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with each character\n",
    "test_prompt = \"How's the Literature Club going?\"\n",
    "\n",
    "print(f\"Test prompt: '{test_prompt}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for character in VN_CHARACTERS.keys():\n",
    "    response = generate_vn_response(\n",
    "        character=character,\n",
    "        user_input=test_prompt,\n",
    "        emotion=\"neutral\",\n",
    "        affection=50\n",
    "    )\n",
    "    print(f\"{character}: {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Define Test Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases organized by character\n",
    "# Format: (user_input, emotion, affection)\n",
    "VN_TEST_CASES = {\n",
    "    'Monika': [\n",
    "        (\"How's the Literature Club going?\", \"neutral\", 30),\n",
    "        (\"I really enjoyed your poem today!\", \"joy\", 60),\n",
    "        (\"You seem stressed about the festival.\", \"neutral\", 45),\n",
    "        (\"What's your favorite book?\", \"curiosity\", 50),\n",
    "        (\"I'm having a tough day...\", \"sadness\", 55),\n",
    "        (\"You're an amazing club president!\", \"joy\", 70),\n",
    "        (\"Do you ever feel lonely?\", \"caring\", 65),\n",
    "        (\"What do you think about the future?\", \"neutral\", 40),\n",
    "        (\"I love spending time with you.\", \"love\", 80),\n",
    "        (\"Would you like to get coffee sometime?\", \"neutral\", 75),\n",
    "    ],\n",
    "    'Sayori': [\n",
    "        (\"You seem happy today!\", \"joy\", 40),\n",
    "        (\"Is everything okay? You seem a bit off...\", \"neutral\", 25),\n",
    "        (\"Your positivity is contagious!\", \"joy\", 60),\n",
    "        (\"What makes you happiest?\", \"curiosity\", 50),\n",
    "        (\"I'm feeling down today.\", \"sadness\", 55),\n",
    "        (\"You're such a good friend.\", \"caring\", 65),\n",
    "        (\"Do you want to hang out after school?\", \"neutral\", 70),\n",
    "        (\"What's your favorite thing about the club?\", \"neutral\", 45),\n",
    "        (\"I really care about you.\", \"love\", 75),\n",
    "        (\"You always know how to cheer me up!\", \"joy\", 80),\n",
    "    ],\n",
    "    'Natsuki': [\n",
    "        (\"What are you reading?\", \"neutral\", 20),\n",
    "        (\"Your manga collection is really impressive!\", \"joy\", 50),\n",
    "        (\"Can you teach me about manga?\", \"curiosity\", 35),\n",
    "        (\"I tried your cupcakes - they're amazing!\", \"joy\", 60),\n",
    "        (\"You're not as tough as you pretend to be.\", \"neutral\", 55),\n",
    "        (\"I think you're really talented.\", \"neutral\", 65),\n",
    "        (\"What's your favorite manga series?\", \"curiosity\", 40),\n",
    "        (\"You seem upset about something.\", \"caring\", 45),\n",
    "        (\"I really like spending time with you.\", \"love\", 70),\n",
    "        (\"Your poetry is actually really good!\", \"joy\", 75),\n",
    "    ],\n",
    "    'Yuri': [\n",
    "        (\"Tell me about your favorite book.\", \"neutral\", 35),\n",
    "        (\"I'd love to hear you read your poetry.\", \"joy\", 65),\n",
    "        (\"What are your hobbies?\", \"curiosity\", 40),\n",
    "        (\"Would you like to go out for tea?\", \"neutral\", 60),\n",
    "        (\"I love your hair.\", \"neutral\", 20),\n",
    "        (\"You're looking beautiful today.\", \"neutral\", 55),\n",
    "        (\"You seem really knowledgeable about literature.\", \"neutral\", 50),\n",
    "        (\"I'm interested in learning more about horror novels.\", \"curiosity\", 70),\n",
    "        (\"You have such elegant taste.\", \"joy\", 75),\n",
    "        (\"I feel comfortable talking to you.\", \"caring\", 80),\n",
    "    ]\n",
    "}\n",
    "\n",
    "total_tests = sum(len(cases) for cases in VN_TEST_CASES.values())\n",
    "print(f\"âœ“ Defined {total_tests} test cases across {len(VN_TEST_CASES)} characters\")\n",
    "for char, cases in VN_TEST_CASES.items():\n",
    "    print(f\"  {char}: {len(cases)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Run Character-Specific Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all test cases and collect responses\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(\"Running VN character tests...\\n\")\n",
    "\n",
    "for character, test_cases in VN_TEST_CASES.items():\n",
    "    print(f\"Testing {character}...\")\n",
    "    \n",
    "    for user_input, emotion, affection in tqdm(test_cases, desc=character):\n",
    "        response = generate_vn_response(\n",
    "            character=character,\n",
    "            user_input=user_input,\n",
    "            emotion=emotion,\n",
    "            affection=affection\n",
    "        )\n",
    "        \n",
    "        all_results.append({\n",
    "            'character': character,\n",
    "            'user_input': user_input,\n",
    "            'emotion': emotion,\n",
    "            'affection': affection,\n",
    "            'response': response,\n",
    "            'response_length': len(response.split())\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(results_df)} responses\")\n",
    "print(f\"\\nSample results:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Basic Dialogue Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response length statistics\n",
    "print(\"=\"*80)\n",
    "print(\"Response Length Statistics\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "length_stats = results_df['response_length'].describe()\n",
    "print(f\"Mean:   {length_stats['mean']:.1f} words\")\n",
    "print(f\"Median: {length_stats['50%']:.1f} words\")\n",
    "print(f\"Min:    {length_stats['min']:.0f} words\")\n",
    "print(f\"Max:    {length_stats['max']:.0f} words\")\n",
    "print(f\"Std:    {length_stats['std']:.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distinct-n metrics\n",
    "def calculate_distinct_n(texts: List[str], n: int) -> float:\n",
    "    \"\"\"Calculate distinct-n score for lexical diversity.\"\"\"\n",
    "    all_ngrams = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = text.lower().split()\n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        all_ngrams.extend(ngrams)\n",
    "    \n",
    "    if len(all_ngrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(set(all_ngrams)) / len(all_ngrams)\n",
    "\n",
    "responses = results_df['response'].tolist()\n",
    "\n",
    "distinct_1 = calculate_distinct_n(responses, 1)\n",
    "distinct_2 = calculate_distinct_n(responses, 2)\n",
    "distinct_3 = calculate_distinct_n(responses, 3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Lexical Diversity (Distinct-N)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Distinct-1 (unique words):    {distinct_1:.4f}\")\n",
    "print(f\"Distinct-2 (unique bigrams):  {distinct_2:.4f}\")\n",
    "print(f\"Distinct-3 (unique trigrams): {distinct_3:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(\"  Good: Distinct-1 > 0.3, Distinct-2 > 0.6\")\n",
    "if distinct_1 > 0.3:\n",
    "    print(\"  âœ“ Good vocabulary diversity\")\n",
    "else:\n",
    "    print(\"  âš  Low vocabulary diversity\")\n",
    "    \n",
    "if distinct_2 > 0.6:\n",
    "    print(\"  âœ“ Good phrase diversity\")\n",
    "else:\n",
    "    print(\"  âš  Low phrase diversity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate repetition ratio\n",
    "def calculate_repetition(text: str) -> float:\n",
    "    \"\"\"Calculate self-repetition ratio.\"\"\"\n",
    "    tokens = text.lower().split()\n",
    "    if len(tokens) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    bigrams = [tuple(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
    "    if len(bigrams) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    repetitions = len(bigrams) - len(set(bigrams))\n",
    "    return repetitions / len(bigrams)\n",
    "\n",
    "results_df['repetition'] = results_df['response'].apply(calculate_repetition)\n",
    "mean_repetition = results_df['repetition'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Self-Repetition Analysis\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Mean Repetition Ratio: {mean_repetition:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "if mean_repetition < 0.05:\n",
    "    print(\"  âœ“ Excellent - Very little repetition\")\n",
    "elif mean_repetition < 0.1:\n",
    "    print(\"  âœ“ Good - Acceptable repetition\")\n",
    "elif mean_repetition < 0.2:\n",
    "    print(\"  âš  Fair - Some repetition present\")\n",
    "else:\n",
    "    print(\"  âš  Poor - High repetition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Character-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-character statistics\n",
    "print(\"=\"*80)\n",
    "print(\"Per-Character Response Quality\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "char_stats = results_df.groupby('character').agg({\n",
    "    'response_length': ['mean', 'std'],\n",
    "    'repetition': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "char_stats.columns = ['Avg Length', 'Std Length', 'Avg Repetition']\n",
    "print(char_stats)\n",
    "\n",
    "# Per-character distinct-n\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Per-Character Lexical Diversity\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "char_diversity = []\n",
    "for character in VN_CHARACTERS.keys():\n",
    "    char_responses = results_df[results_df['character'] == character]['response'].tolist()\n",
    "    char_diversity.append({\n",
    "        'Character': character,\n",
    "        'Distinct-1': calculate_distinct_n(char_responses, 1),\n",
    "        'Distinct-2': calculate_distinct_n(char_responses, 2),\n",
    "        'Distinct-3': calculate_distinct_n(char_responses, 3)\n",
    "    })\n",
    "\n",
    "diversity_df = pd.DataFrame(char_diversity)\n",
    "print(diversity_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Affection Responsiveness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test same prompt at different affection levels\n",
    "affection_test_prompt = \"Would you like to spend some time together?\"\n",
    "affection_levels = [20, 50, 80]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Affection Responsiveness Test\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test prompt: '{affection_test_prompt}'\\n\")\n",
    "\n",
    "affection_results = []\n",
    "\n",
    "for character in VN_CHARACTERS.keys():\n",
    "    print(f\"\\n{character}:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for affection in affection_levels:\n",
    "        response = generate_vn_response(\n",
    "            character=character,\n",
    "            user_input=affection_test_prompt,\n",
    "            emotion=\"neutral\",\n",
    "            affection=affection\n",
    "        )\n",
    "        \n",
    "        print(f\"Affection {affection}/100: {response}\")\n",
    "        \n",
    "        affection_results.append({\n",
    "            'character': character,\n",
    "            'affection': affection,\n",
    "            'response': response\n",
    "        })\n",
    "\n",
    "affection_df = pd.DataFrame(affection_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Character Differentiation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how differently characters respond to identical prompts\n",
    "differentiation_prompts = [\n",
    "    \"What do you think about poetry?\",\n",
    "    \"What's your ideal weekend?\",\n",
    "    \"How do you handle stress?\",\n",
    "    \"What makes you happy?\",\n",
    "    \"Tell me about yourself.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Character Differentiation Test\")\n",
    "print(\"=\"*80)\n",
    "print(\"Testing if characters give distinctly different responses to identical prompts\\n\")\n",
    "\n",
    "differentiation_results = []\n",
    "\n",
    "for prompt in differentiation_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    responses_for_prompt = []\n",
    "    \n",
    "    for character in VN_CHARACTERS.keys():\n",
    "        response = generate_vn_response(\n",
    "            character=character,\n",
    "            user_input=prompt,\n",
    "            emotion=\"neutral\",\n",
    "            affection=50\n",
    "        )\n",
    "        \n",
    "        print(f\"{character}: {response}\")\n",
    "        \n",
    "        responses_for_prompt.append(response)\n",
    "        \n",
    "        differentiation_results.append({\n",
    "            'prompt': prompt,\n",
    "            'character': character,\n",
    "            'response': response\n",
    "        })\n",
    "    \n",
    "    # Calculate uniqueness\n",
    "    unique_words = len(set(' '.join(responses_for_prompt).lower().split()))\n",
    "    total_words = len(' '.join(responses_for_prompt).lower().split())\n",
    "    uniqueness_ratio = unique_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    print(f\"  â†’ Uniqueness ratio: {uniqueness_ratio:.3f}\")\n",
    "\n",
    "differentiation_df = pd.DataFrame(differentiation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Emotion Appropriateness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze emotion distribution in test cases\n",
    "print(\"=\"*80)\n",
    "print(\"Emotion Distribution in Tests\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "emotion_dist = results_df['emotion'].value_counts()\n",
    "print(emotion_dist)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "emotion_dist.plot(kind='bar', ax=ax, color='skyblue')\n",
    "ax.set_xlabel('Emotion', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Emotion Distribution in Test Cases', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Response length by character\n",
    "ax = axes[0, 0]\n",
    "results_df.boxplot(column='response_length', by='character', ax=ax)\n",
    "ax.set_title('Response Length Distribution by Character')\n",
    "ax.set_xlabel('Character')\n",
    "ax.set_ylabel('Words')\n",
    "plt.sca(ax)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 2. Repetition by character\n",
    "ax = axes[0, 1]\n",
    "char_repetition = results_df.groupby('character')['repetition'].mean()\n",
    "char_repetition.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Mean Repetition by Character')\n",
    "ax.set_xlabel('Character')\n",
    "ax.set_ylabel('Repetition Ratio')\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='Target <0.1')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 3. Diversity comparison\n",
    "ax = axes[1, 0]\n",
    "diversity_df.set_index('Character')[['Distinct-1', 'Distinct-2', 'Distinct-3']].plot(\n",
    "    kind='bar', ax=ax\n",
    ")\n",
    "ax.set_title('Lexical Diversity by Character')\n",
    "ax.set_xlabel('Character')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend(title='Metric')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 4. Affection distribution in tests\n",
    "ax = axes[1, 1]\n",
    "results_df['affection'].hist(bins=20, ax=ax, color='lightgreen', edgecolor='black')\n",
    "ax.set_title('Affection Level Distribution in Tests')\n",
    "ax.set_xlabel('Affection (0-100)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.axvline(x=50, color='red', linestyle='--', alpha=0.5, label='Neutral (50)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Problem Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect potential issues\n",
    "print(\"=\"*80)\n",
    "print(\"Problem Detection\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "problems = []\n",
    "\n",
    "# Check for character name confusion\n",
    "for idx, row in results_df.iterrows():\n",
    "    response_lower = row['response'].lower()\n",
    "    character = row['character']\n",
    "    \n",
    "    # Check if response mentions other character names\n",
    "    for other_char in VN_CHARACTERS.keys():\n",
    "        if other_char != character and other_char.lower() in response_lower:\n",
    "            problems.append({\n",
    "                'type': 'character_confusion',\n",
    "                'character': character,\n",
    "                'issue': f\"Mentions '{other_char}' in response\",\n",
    "                'response': row['response'][:100] + \"...\"\n",
    "            })\n",
    "\n",
    "# Check for very short responses\n",
    "short_responses = results_df[results_df['response_length'] < 10]\n",
    "for idx, row in short_responses.iterrows():\n",
    "    problems.append({\n",
    "        'type': 'too_short',\n",
    "        'character': row['character'],\n",
    "        'issue': f\"Only {row['response_length']} words\",\n",
    "        'response': row['response']\n",
    "    })\n",
    "\n",
    "# Check for high repetition\n",
    "high_repetition = results_df[results_df['repetition'] > 0.2]\n",
    "for idx, row in high_repetition.iterrows():\n",
    "    problems.append({\n",
    "        'type': 'high_repetition',\n",
    "        'character': row['character'],\n",
    "        'issue': f\"Repetition ratio: {row['repetition']:.3f}\",\n",
    "        'response': row['response'][:100] + \"...\"\n",
    "    })\n",
    "\n",
    "problems_df = pd.DataFrame(problems)\n",
    "\n",
    "if len(problems_df) > 0:\n",
    "    print(f\"\\nâš ï¸  Found {len(problems_df)} potential issues:\\n\")\n",
    "    \n",
    "    # Group by type\n",
    "    for problem_type, group in problems_df.groupby('type'):\n",
    "        print(f\"\\n{problem_type.upper()} ({len(group)} cases):\")\n",
    "        print(\"-\"*60)\n",
    "        for idx, row in group.head(3).iterrows():\n",
    "            print(f\"  Character: {row['character']}\")\n",
    "            print(f\"  Issue: {row['issue']}\")\n",
    "            print(f\"  Response: {row['response']}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"\\nâœ“ No major issues detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\"*80)\n",
    "print(\"VN MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š OVERALL METRICS\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Total test cases:      {len(results_df)}\")\n",
    "print(f\"Characters tested:     {len(VN_CHARACTERS)}\")\n",
    "print(f\"Avg response length:   {results_df['response_length'].mean():.1f} words\")\n",
    "print(f\"Distinct-1 score:      {distinct_1:.4f}\")\n",
    "print(f\"Distinct-2 score:      {distinct_2:.4f}\")\n",
    "print(f\"Mean repetition:       {mean_repetition:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ‘¥ CHARACTER PERFORMANCE\")\n",
    "print(\"-\"*60)\n",
    "for character in VN_CHARACTERS.keys():\n",
    "    char_data = results_df[results_df['character'] == character]\n",
    "    avg_length = char_data['response_length'].mean()\n",
    "    avg_rep = char_data['repetition'].mean()\n",
    "    print(f\"{character:12s}: Avg {avg_length:4.1f} words, Repetition {avg_rep:.4f}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  ISSUES DETECTED\")\n",
    "print(\"-\"*60)\n",
    "if len(problems_df) > 0:\n",
    "    issue_counts = problems_df['type'].value_counts()\n",
    "    for issue_type, count in issue_counts.items():\n",
    "        print(f\"{issue_type:20s}: {count} cases\")\n",
    "else:\n",
    "    print(\"No major issues detected âœ“\")\n",
    "\n",
    "print(f\"\\nâœ… QUALITY ASSESSMENT\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Overall quality score\n",
    "quality_score = 0\n",
    "max_score = 5\n",
    "\n",
    "if distinct_1 > 0.3:\n",
    "    quality_score += 1\n",
    "    print(\"âœ“ Good vocabulary diversity\")\n",
    "else:\n",
    "    print(\"âœ— Low vocabulary diversity\")\n",
    "\n",
    "if distinct_2 > 0.6:\n",
    "    quality_score += 1\n",
    "    print(\"âœ“ Good phrase diversity\")\n",
    "else:\n",
    "    print(\"âœ— Low phrase diversity\")\n",
    "\n",
    "if mean_repetition < 0.1:\n",
    "    quality_score += 1\n",
    "    print(\"âœ“ Low repetition\")\n",
    "else:\n",
    "    print(\"âœ— High repetition\")\n",
    "\n",
    "if len(problems_df[problems_df['type'] == 'character_confusion']) == 0:\n",
    "    quality_score += 1\n",
    "    print(\"âœ“ No character confusion detected\")\n",
    "else:\n",
    "    print(\"âœ— Character confusion present\")\n",
    "\n",
    "if results_df['response_length'].mean() > 15:\n",
    "    quality_score += 1\n",
    "    print(\"âœ“ Adequate response length\")\n",
    "else:\n",
    "    print(\"âœ— Responses too short\")\n",
    "\n",
    "print(f\"\\nOverall Quality Score: {quality_score}/{max_score}\")\n",
    "\n",
    "if quality_score >= 4:\n",
    "    print(\"ðŸŽ‰ Excellent model performance!\")\n",
    "elif quality_score >= 3:\n",
    "    print(\"ðŸ‘ Good model performance with room for improvement\")\n",
    "else:\n",
    "    print(\"âš ï¸  Model needs significant improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = Path(\"../../results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save detailed results\n",
    "results_path = results_dir / f\"vn_evaluation_{timestamp}.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"âœ“ Detailed results saved to: {results_path}\")\n",
    "\n",
    "# Save affection test results\n",
    "affection_path = results_dir / f\"vn_affection_test_{timestamp}.csv\"\n",
    "affection_df.to_csv(affection_path, index=False)\n",
    "print(f\"âœ“ Affection test saved to: {affection_path}\")\n",
    "\n",
    "# Save differentiation test results\n",
    "diff_path = results_dir / f\"vn_differentiation_test_{timestamp}.csv\"\n",
    "differentiation_df.to_csv(diff_path, index=False)\n",
    "print(f\"âœ“ Differentiation test saved to: {diff_path}\")\n",
    "\n",
    "# Save summary metrics\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'model': VN_CHECKPOINT,\n",
    "    'total_tests': len(results_df),\n",
    "    'characters': list(VN_CHARACTERS.keys()),\n",
    "    'metrics': {\n",
    "        'avg_response_length': float(results_df['response_length'].mean()),\n",
    "        'distinct_1': float(distinct_1),\n",
    "        'distinct_2': float(distinct_2),\n",
    "        'distinct_3': float(distinct_3),\n",
    "        'mean_repetition': float(mean_repetition)\n",
    "    },\n",
    "    'per_character': char_stats.to_dict(),\n",
    "    'issues': problems_df.to_dict('records') if len(problems_df) > 0 else [],\n",
    "    'quality_score': f\"{quality_score}/{max_score}\"\n",
    "}\n",
    "\n",
    "summary_path = results_dir / f\"vn_evaluation_summary_{timestamp}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"âœ“ Summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Evaluation Complete! ðŸŽ‰\n",
    "\n",
    "### Key Findings:\n",
    "- Review the summary above for overall performance\n",
    "- Check character-specific metrics for individual performance\n",
    "- Examine problem detection for areas needing improvement\n",
    "\n",
    "### Next Steps:\n",
    "1. **If quality score < 3:** Consider retraining with more data or different hyperparameters\n",
    "2. **If character confusion detected:** Filter training data to single-character conversations\n",
    "3. **If repetition high:** Increase repetition penalty in generation\n",
    "4. **If diversity low:** Check training data diversity, consider data augmentation\n",
    "\n",
    "### Files Generated:\n",
    "- `results/vn_evaluation_TIMESTAMP.csv` - Detailed test results\n",
    "- `results/vn_affection_test_TIMESTAMP.csv` - Affection responsiveness\n",
    "- `results/vn_differentiation_test_TIMESTAMP.csv` - Character differentiation\n",
    "- `results/vn_evaluation_summary_TIMESTAMP.json` - Complete summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
