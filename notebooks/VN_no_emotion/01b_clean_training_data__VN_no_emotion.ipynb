{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VN Training Data Cleaning - Remove Multi-Character Dialogue\n",
    "\n",
    "**Purpose:** Clean VN training data to remove multi-character conversations and formatting artifacts\n",
    "\n",
    "**Problem Identified:**\n",
    "- Training data contains multi-character dialogue scenes (not single-character conversations)\n",
    "- Assistant responses mention other characters (Monika, Sayori, Natsuki, Yuri)\n",
    "- VN formatting artifacts present: `{i}...{/i}`, `{b}...{/b}`, etc.\n",
    "- Model learns to continue multi-character scenes instead of single responses\n",
    "\n",
    "**What this notebook does:**\n",
    "1. ğŸ“š Load raw VN JSONL data (all 4 characters)\n",
    "2. ğŸ” Identify multi-character conversations\n",
    "3. ğŸ§¹ Filter out examples where:\n",
    "   - User mentions other character names\n",
    "   - Assistant mentions other character names\n",
    "4. âœ¨ Clean formatting artifacts:\n",
    "   - Remove `{i}`, `{/i}`, `{b}`, `{/b}`, etc.\n",
    "   - Keep `<USER>` placeholder (intentional for inference)\n",
    "   - Clean excessive ellipsis\n",
    "5. ğŸ“Š Generate before/after statistics\n",
    "6. ğŸ’¾ Save cleaned data to `data/processed/VN/cleaned/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: ../../data/processed/VN_no_emotion\n",
      "Output directory: ../../data/processed/VN_no_emotion/processed\n",
      "Characters to process: Monika, Sayori, Natsuki, Yuri\n"
     ]
    }
   ],
   "source": [
    "# Paths (NO EMOTION version)\n",
    "VN_DATA_DIR = Path(\"../../data/processed/VN_no_emotion\")\n",
    "OUTPUT_DIR = Path(\"../../data/processed/VN_no_emotion/processed\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Characters\n",
    "VN_CHARACTERS = ['Monika', 'Sayori', 'Natsuki', 'Yuri']\n",
    "\n",
    "print(f\"Input directory: {VN_DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Characters to process: {', '.join(VN_CHARACTERS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Cleaning functions defined\n"
     ]
    }
   ],
   "source": [
    "def clean_text_formatting(text):\n",
    "    \"\"\"\n",
    "    Remove VN formatting artifacts from text.\n",
    "    \n",
    "    Removes:\n",
    "    - {i}, {/i}, {b}, {/b}, etc. (VN markup)\n",
    "    - Excessive ellipsis (more than 3 dots)\n",
    "    - Extra whitespace\n",
    "    \n",
    "    Keeps:\n",
    "    - <USER> placeholder (intentional for inference)\n",
    "    \"\"\"\n",
    "    # Remove VN markup tags\n",
    "    text = re.sub(r'\\{/?i\\}', '', text)  # {i} and {/i}\n",
    "    text = re.sub(r'\\{/?b\\}', '', text)  # {b} and {/b}\n",
    "    text = re.sub(r'\\{/?u\\}', '', text)  # {u} and {/u}\n",
    "    text = re.sub(r'\\{/?s\\}', '', text)  # {s} and {/s}\n",
    "    text = re.sub(r'\\{/?color[^}]*\\}', '', text)  # {color=...} and {/color}\n",
    "    text = re.sub(r'\\{/?size[^}]*\\}', '', text)  # {size=...} and {/size}\n",
    "    \n",
    "    # Clean excessive ellipsis (keep max 3 dots)\n",
    "    text = re.sub(r'\\.{4,}', '...', text)\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def contains_other_characters(text, current_character, characters_list):\n",
    "    \"\"\"\n",
    "    Check if text mentions other character names.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to check\n",
    "        current_character: The character this conversation is about\n",
    "        characters_list: List of all character names\n",
    "    \n",
    "    Returns:\n",
    "        True if other character names are found, False otherwise\n",
    "    \"\"\"\n",
    "    other_characters = [c for c in characters_list if c != current_character]\n",
    "    \n",
    "    for other_char in other_characters:\n",
    "        # Case-insensitive search for character names\n",
    "        if re.search(rf'\\b{re.escape(other_char)}\\b', text, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def should_keep_example(messages, current_character, characters_list):\n",
    "    \"\"\"\n",
    "    Determine if a conversation example should be kept.\n",
    "    \n",
    "    Filter out if:\n",
    "    - User turns mention other character names\n",
    "    - Assistant turns mention other character names\n",
    "    - No assistant responses present\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "        current_character: The character this conversation is about\n",
    "        characters_list: List of all character names\n",
    "    \n",
    "    Returns:\n",
    "        (keep: bool, reason: str)\n",
    "    \"\"\"\n",
    "    has_assistant = False\n",
    "    \n",
    "    for msg in messages:\n",
    "        role = msg.get('role', '')\n",
    "        content = msg.get('content', '')\n",
    "        \n",
    "        if role == 'assistant':\n",
    "            has_assistant = True\n",
    "            # Check if assistant mentions other characters\n",
    "            if contains_other_characters(content, current_character, characters_list):\n",
    "                return False, \"assistant mentions other characters\"\n",
    "        \n",
    "        elif role == 'user':\n",
    "            # Check if user mentions other characters\n",
    "            if contains_other_characters(content, current_character, characters_list):\n",
    "                return False, \"user mentions other characters\"\n",
    "    \n",
    "    if not has_assistant:\n",
    "        return False, \"no assistant responses\"\n",
    "    \n",
    "    return True, \"OK\"\n",
    "\n",
    "\n",
    "def clean_example(example, current_character):\n",
    "    \"\"\"\n",
    "    Clean formatting artifacts from all messages in an example.\n",
    "    \n",
    "    Args:\n",
    "        example: Dict with 'messages' key\n",
    "        current_character: Character name for this conversation\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned example dict\n",
    "    \"\"\"\n",
    "    cleaned_messages = []\n",
    "    \n",
    "    for msg in example['messages']:\n",
    "        cleaned_msg = msg.copy()\n",
    "        cleaned_msg['content'] = clean_text_formatting(msg['content'])\n",
    "        cleaned_messages.append(cleaned_msg)\n",
    "    \n",
    "    return {'messages': cleaned_messages}\n",
    "\n",
    "\n",
    "print(\"âœ“ Cleaning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Text Cleaning Test\n",
      "================================================================================\n",
      "\n",
      "Original: Welcome to the {i}Literature Club{/i}! It's a pleasure meeting you.\n",
      "Cleaned:  Welcome to the Literature Club! It's a pleasure meeting you.\n",
      "\n",
      "Original: I-I don't mind... I just need some time........ to think before making any decisions--\n",
      "Cleaned:  I-I don't mind... I just need some time... to think before making any decisions--\n",
      "\n",
      "Original: Natsuki! What did you just say...?!\n",
      "Cleaned:  Natsuki! What did you just say...?!\n",
      "\n",
      "Original: The level of {b}creativity{/b} and craftsmanship behind them is amazing.\n",
      "Cleaned:  The level of creativity and craftsmanship behind them is amazing.\n",
      "\n",
      "================================================================================\n",
      "Character Mention Detection Test\n",
      "================================================================================\n",
      "\n",
      "âœ“ Text: 'Welcome to the Literature Club!'\n",
      "  Character: Yuri\n",
      "  Expected: False, Got: False\n",
      "\n",
      "âœ“ Text: 'Natsuki will be performing tonight!!'\n",
      "  Character: Sayori\n",
      "  Expected: True, Got: True\n",
      "\n",
      "âœ“ Text: 'I think monika would like this'\n",
      "  Character: Yuri\n",
      "  Expected: True, Got: True\n",
      "\n",
      "âœ“ Text: 'What do you think about my poem?'\n",
      "  Character: Monika\n",
      "  Expected: False, Got: False\n"
     ]
    }
   ],
   "source": [
    "# Test text cleaning\n",
    "test_texts = [\n",
    "    \"Welcome to the {i}Literature Club{/i}! It's a pleasure meeting you.\",\n",
    "    \"I-I don't mind... I just need some time........ to think before making any decisions--\",\n",
    "    \"Natsuki! What did you just say...?!\",\n",
    "    \"The level of {b}creativity{/b} and craftsmanship behind them is amazing.\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Text Cleaning Test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in test_texts:\n",
    "    cleaned = clean_text_formatting(text)\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "\n",
    "# Test character detection\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Character Mention Detection Test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Welcome to the Literature Club!\", \"Yuri\", False),\n",
    "    (\"Natsuki will be performing tonight!!\", \"Sayori\", True),\n",
    "    (\"I think monika would like this\", \"Yuri\", True),\n",
    "    (\"What do you think about my poem?\", \"Monika\", False),\n",
    "]\n",
    "\n",
    "for text, char, expected in test_cases:\n",
    "    result = contains_other_characters(text, char, VN_CHARACTERS)\n",
    "    status = \"âœ“\" if result == expected else \"âœ—\"\n",
    "    print(f\"\\n{status} Text: '{text}'\")\n",
    "    print(f\"  Character: {char}\")\n",
    "    print(f\"  Expected: {expected}, Got: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VN training data...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Processing Monika...\n",
      "  Original: 110 examples\n",
      "  Kept:     29 examples (26.4%)\n",
      "  Filtered: 81 examples (73.6%)\n",
      "  Reasons:\n",
      "    - user mentions other characters: 48\n",
      "    - assistant mentions other characters: 33\n",
      "  âœ“ Saved to: ../../data/processed/VN_no_emotion/processed/vn_training_data_Monika_cleaned.jsonl\n",
      "\n",
      "Processing Sayori...\n",
      "  Original: 90 examples\n",
      "  Kept:     49 examples (54.4%)\n",
      "  Filtered: 41 examples (45.6%)\n",
      "  Reasons:\n",
      "    - assistant mentions other characters: 18\n",
      "    - user mentions other characters: 23\n",
      "  âœ“ Saved to: ../../data/processed/VN_no_emotion/processed/vn_training_data_Sayori_cleaned.jsonl\n",
      "\n",
      "Processing Natsuki...\n",
      "  Original: 111 examples\n",
      "  Kept:     52 examples (46.8%)\n",
      "  Filtered: 59 examples (53.2%)\n",
      "  Reasons:\n",
      "    - user mentions other characters: 42\n",
      "    - assistant mentions other characters: 17\n",
      "  âœ“ Saved to: ../../data/processed/VN_no_emotion/processed/vn_training_data_Natsuki_cleaned.jsonl\n",
      "\n",
      "Processing Yuri...\n",
      "  Original: 128 examples\n",
      "  Kept:     70 examples (54.7%)\n",
      "  Filtered: 58 examples (45.3%)\n",
      "  Reasons:\n",
      "    - assistant mentions other characters: 19\n",
      "    - user mentions other characters: 39\n",
      "  âœ“ Saved to: ../../data/processed/VN_no_emotion/processed/vn_training_data_Yuri_cleaned.jsonl\n",
      "\n",
      "================================================================================\n",
      "âœ“ Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Statistics tracking\n",
    "stats = {\n",
    "    'total_before': 0,\n",
    "    'total_after': 0,\n",
    "    'filtered_reasons': defaultdict(int),\n",
    "    'by_character': {}\n",
    "}\n",
    "\n",
    "print(\"Processing VN training data...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for character in VN_CHARACTERS:\n",
    "    print(f\"\\nProcessing {character}...\")\n",
    "    \n",
    "    # Load data\n",
    "    input_file = VN_DATA_DIR / f\"vn_training_data_{character}.jsonl\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    original_count = len(data)\n",
    "    stats['total_before'] += original_count\n",
    "    \n",
    "    # Filter and clean\n",
    "    kept_examples = []\n",
    "    char_filtered_reasons = defaultdict(int)\n",
    "    \n",
    "    for example in data:\n",
    "        keep, reason = should_keep_example(example['messages'], character, VN_CHARACTERS)\n",
    "        \n",
    "        if keep:\n",
    "            # Clean the example\n",
    "            cleaned = clean_example(example, character)\n",
    "            kept_examples.append(cleaned)\n",
    "        else:\n",
    "            char_filtered_reasons[reason] += 1\n",
    "            stats['filtered_reasons'][reason] += 1\n",
    "    \n",
    "    kept_count = len(kept_examples)\n",
    "    stats['total_after'] += kept_count\n",
    "    filtered_count = original_count - kept_count\n",
    "    \n",
    "    # Save cleaned data\n",
    "    output_file = OUTPUT_DIR / f\"vn_training_data_{character}_cleaned.jsonl\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for example in kept_examples:\n",
    "            f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # Store character stats\n",
    "    stats['by_character'][character] = {\n",
    "        'original': original_count,\n",
    "        'kept': kept_count,\n",
    "        'filtered': filtered_count,\n",
    "        'filtered_reasons': dict(char_filtered_reasons)\n",
    "    }\n",
    "    \n",
    "    # Print character summary\n",
    "    print(f\"  Original: {original_count} examples\")\n",
    "    print(f\"  Kept:     {kept_count} examples ({kept_count/original_count*100:.1f}%)\")\n",
    "    print(f\"  Filtered: {filtered_count} examples ({filtered_count/original_count*100:.1f}%)\")\n",
    "    \n",
    "    if char_filtered_reasons:\n",
    "        print(f\"  Reasons:\")\n",
    "        for reason, count in char_filtered_reasons.items():\n",
    "            print(f\"    - {reason}: {count}\")\n",
    "    \n",
    "    print(f\"  âœ“ Saved to: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OVERALL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total examples:\n",
      "  Before: 439\n",
      "  After:  200\n",
      "  Filtered: 239 (54.4%)\n",
      "\n",
      "Filtering reasons (overall):\n",
      "  user mentions other characters: 152 (34.6%)\n",
      "  assistant mentions other characters: 87 (19.8%)\n",
      "\n",
      "Per-character breakdown:\n",
      "  Monika      :  29/110 kept (26.4% retention)\n",
      "  Sayori      :  49/ 90 kept (54.4% retention)\n",
      "  Natsuki     :  52/111 kept (46.8% retention)\n",
      "  Yuri        :  70/128 kept (54.7% retention)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal examples:\")\n",
    "print(f\"  Before: {stats['total_before']}\")\n",
    "print(f\"  After:  {stats['total_after']}\")\n",
    "print(f\"  Filtered: {stats['total_before'] - stats['total_after']} ({(stats['total_before'] - stats['total_after'])/stats['total_before']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFiltering reasons (overall):\")\n",
    "for reason, count in sorted(stats['filtered_reasons'].items(), key=lambda x: -x[1]):\n",
    "    pct = count / stats['total_before'] * 100\n",
    "    print(f\"  {reason}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPer-character breakdown:\")\n",
    "for character in VN_CHARACTERS:\n",
    "    char_stats = stats['by_character'][character]\n",
    "    retention_rate = char_stats['kept'] / char_stats['original'] * 100\n",
    "    print(f\"  {character:12s}: {char_stats['kept']:3d}/{char_stats['original']:3d} kept ({retention_rate:.1f}% retention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE CLEANED CONVERSATIONS\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Monika - Sample Cleaned Conversations\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Example 1:\n",
      "  system    : You are Monika, the Literature Club president. Confident, intelligent, and caring. You're thoughtful...\n",
      "  user      : Don't make promises you can't keep! Fine... I'll stop by for a cupcake, okay? I told you, don't call...\n",
      "  assistant : Ah, <USER>! What a nice surprise! Welcome to the club!\n",
      "  user      : ... S-Sorry... Ah... Well, it's nice to meet both of you.\n",
      "  assistant : That's right. It's great to see you again, <USER>.\n",
      "  user      : Y-<USER> too, Monika.\n",
      "\n",
      "Example 2:\n",
      "  system    : You are Monika, the Literature Club president. Confident, intelligent, and caring. You're thoughtful...\n",
      "  user      : Okay, I've decided, then. I'll join the Literature Club. H-Hey--\n",
      "  assistant : Welcome to the Literature Club!\n",
      "  user      : Ah...thanks, I guess.\n",
      "  assistant : I think with that, we can officially end today's meeting on a good note. Everyone remember tonight's...\n",
      "  user      : Y-Yeah... Sure, might as well.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Sayori - Sample Cleaned Conversations\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Example 1:\n",
      "  system    : You are Sayori, a cheerful childhood friend. Bubbly, energetic, and optimistic, though you hide deep...\n",
      "  assistant : Heeeeeeeyyy!! I overslept again! But I caught you this time!\n",
      "  user      : Maybe, but only because I decided to stop and wait for you.\n",
      "  assistant : That's mean, <USER>!\n",
      "  user      : Well, if people stare at you for acting weird then I don't want them to think we're a couple or some...\n",
      "  assistant : But you did wait for me, after all. I guess you don't have it in you to be mean even if you want to~\n",
      "  user      : Whatever you say, Sayori...\n",
      "  assistant : By the way, <USER>... Have you decided on a club to join yet?\n",
      "  user      : A club? I told you already, I'm really not interested in joining any clubs. I haven't been looking, ...\n",
      "  assistant : You told me you would join a club this year!\n",
      "  user      : Did I...?\n",
      "  assistant : I was talking about how I'm worried that you won't learn how to socialize or have any skills before ...\n",
      "  user      : Alright, alright... I'll look at a few clubs if it makes you happy. No promises, though.\n",
      "\n",
      "Example 2:\n",
      "  system    : You are Sayori, a cheerful childhood friend. Bubbly, energetic, and optimistic, though you hide deep...\n",
      "  user      : Okay, I've decided, then. I'll join the Literature Club. H-Hey-- Ah...thanks, I guess. Y-Yeah...\n",
      "  assistant : Hey, <USER>, since we're already here, do you want to walk home together?\n",
      "  user      : Sure, might as well.\n",
      "  assistant : ... ... W-What... This... What is this...? Oh no... This can't be it. This can't be all there is. Wh...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Natsuki - Sample Cleaned Conversations\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Example 1:\n",
      "  system    : You are Natsuki, a tsundere who loves manga and baking. Defensive exterior but sweet underneath. Fei...\n",
      "  assistant : I wasn't the one whose boobs magically grew a size bigger as soon as <USER> started showing up!! She...\n",
      "  user      : Um...! ...\n",
      "  assistant : It was alright. Well, mostly.\n",
      "  user      : ...Yeah, I'd say the same.\n",
      "\n",
      "Example 2:\n",
      "  system    : You are Natsuki, a tsundere who loves manga and baking. Defensive exterior but sweet underneath. Fei...\n",
      "  assistant : Excuse me?!\n",
      "  user      : Ah, there you are... Ah, never mind that... What held you up, anyway?\n",
      "  assistant : That makes no sense, though. You would have heard the bell ring, at least.\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Yuri - Sample Cleaned Conversations\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "Example 1:\n",
      "  system    : You are Yuri, shy and sophisticated with a passion for literature. Elegant but socially anxious. Int...\n",
      "  assistant : We'll do our best. So, <USER>, what kinds of things do you like to read?\n",
      "  user      : Well... Ah... ...Manga... ...Well, that can change... Anyway, what about you, Yuri?\n",
      "  assistant : The level of creativity and craftsmanship behind them is amazing to me. Stories with deep psychologi...\n",
      "  user      : Ah, I read a horror book once...\n",
      "  assistant : But if a story makes me think, or takes me to another world, then I really can't put it down. Surrea...\n",
      "\n",
      "Example 2:\n",
      "  system    : You are Yuri, shy and sophisticated with a passion for literature. Elegant but socially anxious. Int...\n",
      "  assistant : Alright... I didn't want you to feel left out... So I picked out a book that I thought you might enj...\n",
      "  user      : Yuri, thank you! I'll definitely read this!\n",
      "  assistant : Phew... I look forward to hearing what you think.\n",
      "  user      : Y-Yeah... Phew...\n",
      "  assistant : Um...did you say something? Yours is...cute... I just meant... I was trying to say something nice......\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAMPLE CLEANED CONVERSATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show 2 examples from each character\n",
    "for character in VN_CHARACTERS:\n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(f\"{character} - Sample Cleaned Conversations\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    \n",
    "    # Load cleaned data\n",
    "    cleaned_file = OUTPUT_DIR / f\"vn_training_data_{character}_cleaned.jsonl\"\n",
    "    \n",
    "    with open(cleaned_file, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Show first 2 examples\n",
    "    for i, example in enumerate(cleaned_data[:2], 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        for msg in example['messages']:\n",
    "            role = msg['role']\n",
    "            content = msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content']\n",
    "            print(f\"  {role:10s}: {content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Statistics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Statistics saved to: ../../data/processed/VN_no_emotion/processed/cleaning_statistics.json\n",
      "âœ“ Summary saved to: ../../data/processed/VN_no_emotion/processed/CLEANING_SUMMARY.txt\n",
      "\n",
      "================================================================================\n",
      "âœ… DATA CLEANING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Cleaned data location: ../../data/processed/VN_no_emotion/processed\n",
      "Total cleaned examples: 200\n",
      "\n",
      "Next steps:\n",
      "1. Review the cleaned data samples above\n",
      "2. Update training notebook (03_complete_training__VN.ipynb):\n",
      "   Change: VN_DATA_DIR = '../../data/processed/VN/cleaned'\n",
      "3. Retrain model with cleaned data\n",
      "4. Test generation with updated parameters (max_new_tokens=50)\n"
     ]
    }
   ],
   "source": [
    "# Save statistics to JSON\n",
    "stats_file = OUTPUT_DIR / \"cleaning_statistics.json\"\n",
    "\n",
    "# Convert defaultdict to regular dict for JSON serialization\n",
    "stats_to_save = {\n",
    "    'total_before': stats['total_before'],\n",
    "    'total_after': stats['total_after'],\n",
    "    'filtered_reasons': dict(stats['filtered_reasons']),\n",
    "    'by_character': stats['by_character']\n",
    "}\n",
    "\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats_to_save, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Statistics saved to: {stats_file}\")\n",
    "\n",
    "# Create summary text file\n",
    "summary_file = OUTPUT_DIR / \"CLEANING_SUMMARY.txt\"\n",
    "\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"VN TRAINING DATA CLEANING SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"OVERALL STATISTICS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Total examples before: {stats['total_before']}\\n\")\n",
    "    f.write(f\"Total examples after:  {stats['total_after']}\\n\")\n",
    "    f.write(f\"Filtered:              {stats['total_before'] - stats['total_after']} ({(stats['total_before'] - stats['total_after'])/stats['total_before']*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"FILTERING REASONS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for reason, count in sorted(stats['filtered_reasons'].items(), key=lambda x: -x[1]):\n",
    "        pct = count / stats['total_before'] * 100\n",
    "        f.write(f\"  {reason}: {count} ({pct:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nPER-CHARACTER BREAKDOWN\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for character in VN_CHARACTERS:\n",
    "        char_stats = stats['by_character'][character]\n",
    "        retention_rate = char_stats['kept'] / char_stats['original'] * 100\n",
    "        f.write(f\"\\n{character}:\\n\")\n",
    "        f.write(f\"  Original: {char_stats['original']}\\n\")\n",
    "        f.write(f\"  Kept:     {char_stats['kept']} ({retention_rate:.1f}% retention)\\n\")\n",
    "        f.write(f\"  Filtered: {char_stats['filtered']}\\n\")\n",
    "        \n",
    "        if char_stats['filtered_reasons']:\n",
    "            f.write(f\"  Reasons:\\n\")\n",
    "            for reason, count in char_stats['filtered_reasons'].items():\n",
    "                f.write(f\"    - {reason}: {count}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    f.write(\"\\nNEXT STEPS:\\n\")\n",
    "    f.write(\"1. Review sample cleaned conversations above\\n\")\n",
    "    f.write(\"2. Update training notebook to use cleaned data:\\n\")\n",
    "    f.write(\"   VN_DATA_DIR = '../../data/processed/VN/cleaned'\\n\")\n",
    "    f.write(\"3. Retrain model with cleaned data\\n\")\n",
    "    f.write(\"4. Verify character blending is reduced\\n\")\n",
    "\n",
    "print(f\"âœ“ Summary saved to: {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… DATA CLEANING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCleaned data location: {OUTPUT_DIR}\")\n",
    "print(f\"Total cleaned examples: {stats['total_after']}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"1. Review the cleaned data samples above\")\n",
    "print(\"2. Update training notebook (03_complete_training__VN.ipynb):\")\n",
    "print(\"   Change: VN_DATA_DIR = '../../data/processed/VN/cleaned'\")\n",
    "print(\"3. Retrain model with cleaned data\")\n",
    "print(\"4. Test generation with updated parameters (max_new_tokens=50)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
